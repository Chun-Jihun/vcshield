{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "C:\\Users\\mmc\\AppData\\Local\\Temp\\ipykernel_53896\\673856426.py:7: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier, HIFIGAN\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "from speechbrain.pretrained import EncoderClassifier, HIFIGAN\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader (길이 반환) 준비 완료.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. (이전 단계) 데이터로더 설정 ---\n",
    "# (CustomLibriSpeechDataset, collate_fn_custom_with_lengths 정의 필요)\n",
    "class CustomLibriSpeechDataset(Dataset):\n",
    "    \"\"\"\n",
    "    사용자의 디렉터리 구조(예: ./data/train/dev-clean)를 \n",
    "    직접 읽어오는 커스텀 데이터셋 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir):\n",
    "        # LibriSpeech는 .flac 확장자를 사용합니다.\n",
    "        self.file_paths = sorted(list(Path(root_dir).rglob(\"*.flac\")))\n",
    "        \n",
    "        if not self.file_paths:\n",
    "            raise FileNotFoundError(f\"'{root_dir}'에서 .flac 파일을 찾을 수 없습니다. 경로를 확인하세요.\")\n",
    "            \n",
    "        # 화자 ID(문자열)를 정수(integer)로 매핑합니다.\n",
    "        self.speaker_map = {}\n",
    "        self.file_list_with_speaker_id = []\n",
    "        \n",
    "        speaker_counter = 0\n",
    "        \n",
    "        for file_path in self.file_paths:\n",
    "            # 파일 경로에서 화자 ID 추출\n",
    "            # 예: .../dev-clean/84/121123/84-121123.flac\n",
    "            #       -> 부모의 부모 폴더 이름('84')이 화자 ID\n",
    "            speaker_id_str = file_path.parent.parent.name\n",
    "            \n",
    "            # 화자 ID를 정수로 변환\n",
    "            if speaker_id_str not in self.speaker_map:\n",
    "                self.speaker_map[speaker_id_str] = speaker_counter\n",
    "                speaker_counter += 1\n",
    "            \n",
    "            speaker_id_int = self.speaker_map[speaker_id_str]\n",
    "            \n",
    "            # (파일 경로, 정수형 화자 ID) 튜플로 저장\n",
    "            self.file_list_with_speaker_id.append((str(file_path), speaker_id_int))\n",
    "            \n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 샘플 수\n",
    "        return len(self.file_list_with_speaker_id)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # idx번째 샘플을 반환\n",
    "        file_path, speaker_id = self.file_list_with_speaker_id[idx]\n",
    "        \n",
    "        # 오디오 로드\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        # (파형, 샘플레이트, 화자ID) 반환\n",
    "        return waveform, sample_rate, speaker_id\n",
    "\n",
    "def collate_fn_custom_with_lengths(batch):\n",
    "    \"\"\"\n",
    "    길이가 다른 오디오 샘플들을 묶고, '실제 길이'를 반환하는 함수\n",
    "    \"\"\"\n",
    "    waveforms = []\n",
    "    speaker_ids = []\n",
    "    lengths = []  # <-- [추가] 실제 길이를 저장할 리스트\n",
    "    \n",
    "    for sample in batch:\n",
    "        waveform, sample_rate, speaker_id = sample\n",
    "        \n",
    "        waveforms.append(waveform.squeeze(0)) # (1, Length) -> (Length)\n",
    "        speaker_ids.append(speaker_id)\n",
    "        lengths.append(waveform.shape[1]) # <-- [추가] 패딩 전 실제 길이\n",
    "        \n",
    "    # 패딩(Padding) 처리\n",
    "    waveforms_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        waveforms, batch_first=True, padding_value=0.\n",
    "    )\n",
    "    \n",
    "    # 화자 ID와 길이를 텐서로 변환\n",
    "    speaker_ids_tensor = torch.tensor(speaker_ids, dtype=torch.int64)\n",
    "    lengths_tensor = torch.tensor(lengths, dtype=torch.int64) # <-- [추가]\n",
    "    \n",
    "    # (패딩된 파형, 화자 ID, 실제 길이 텐서) 반환\n",
    "    return waveforms_padded, speaker_ids_tensor, lengths_tensor\n",
    "\n",
    "data_root_dev = \"./data/train/dev-clean\"\n",
    "batch_size = 1\n",
    "try:\n",
    "    librispeech_dataset_dev = CustomLibriSpeechDataset(root_dir=data_root_dev)\n",
    "    dev_loader = DataLoader(\n",
    "        dataset=librispeech_dataset_dev,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn_custom_with_lengths\n",
    "    )\n",
    "    print(\"DataLoader (길이 반환) 준비 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"[치명적 오류] 1단계 데이터셋 로드 실패: {e}\")\n",
    "    # (Jupyter Notebook에서는 exit() 대신 SystemExit 예외를 발생)\n",
    "    raise SystemExit(\"데이터로더 생성 실패.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\site-packages\\speechbrain\\utils\\autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공격 대상 모델 (ECAPA-TDNN) 로드 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴럴 보코더 (HiFi-GAN LibriTTS-16kHz) 로드 완료.\n",
      "SpeechBrain 전용 멜 스펙트로그램 함수 로드 완료.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 모델 및 변환기 로드 ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2-1. 화자 인코더 (ECAPA-TDNN, 16kHz)\n",
    "try:\n",
    "    speaker_model = EncoderClassifier.from_hparams(\n",
    "        source=\"speechbrain/spkrec-ecapa-voxceleb\", \n",
    "        run_opts={\"device\": device}\n",
    "    )\n",
    "    speaker_model.eval()\n",
    "    print(\"공격 대상 모델 (ECAPA-TDNN) 로드 완료.\")\n",
    "    \n",
    "    # [수정] 모듈 분리\n",
    "    feature_extractor = speaker_model.mods.compute_features\n",
    "    embedding_model = speaker_model.mods.embedding_model\n",
    "    feature_extractor.eval()\n",
    "    embedding_model.eval()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[치명적 오류] 2-1단계 화자 인코더 로드 실패: {e}\")\n",
    "    raise SystemExit(\"speaker_model 로드 실패.\")\n",
    "\n",
    "# 2-2. 뉴럴 보코더 (HiFi-GAN, LibriTTS 16kHz)\n",
    "try:\n",
    "    vocoder = HIFIGAN.from_hparams(\n",
    "        source=\"speechbrain/tts-hifigan-libritts-16kHz\", # <-- [수정됨]\n",
    "        run_opts={\"device\": device}\n",
    "    )\n",
    "    vocoder.eval()\n",
    "    print(\"뉴럴 보코더 (HiFi-GAN LibriTTS-16kHz) 로드 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"[치명적 오류] 2-2단계 보코더 로드 실패: {e}\")\n",
    "    raise SystemExit(\"vocoder 로드 실패.\")\n",
    "\n",
    "# --- 2-3. 표준 멜 스펙트로그램 변환기 (수정됨) ---\n",
    "try:\n",
    "    from speechbrain.lobes.models.FastSpeech2 import mel_spectogram\n",
    "    print(\"SpeechBrain 전용 멜 스펙트로그램 함수 로드 완료.\")\n",
    "except ImportError:\n",
    "    print(\"[치명적 오류] 'speechbrain.lobes.models.FastSpeech2'에서 'mel_spectogram'을 찾을 수 없습니다.\")\n",
    "    raise SystemExit(\"SpeechBrain 멜 함수 로드 실패.\")\n",
    "\n",
    "# 보코더 훈련에 사용된 정확한 파라미터\n",
    "SAMPLE_RATE = 16000\n",
    "HOP_LENGTH = 256\n",
    "WIN_LENGTH = 1024\n",
    "N_MELS = 80\n",
    "N_FFT = 1024\n",
    "F_MIN = 0.0\n",
    "F_MAX = 8000.0\n",
    "POWER = 1.0 \n",
    "COMPRESSION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3단계: 데이터로더에서 배치 가져오는 중...\n",
      "배치 로드 완료. 파형 Shape: torch.Size([1, 77920])\n",
      "원본 임베딩(타겟) Shape: torch.Size([1, 192])\n",
      "원본 멜(공격대상) Shape: torch.Size([1, 80, 305])\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 원본 데이터 및 \"기준점\" 추출 (수정됨) ---\n",
    "print(\"\\n3단계: 데이터로더에서 배치 가져오는 중...\")\n",
    "try:\n",
    "    # (dev_loader가 이미 정의되어 있으므로 next()만 다시 호출)\n",
    "    waveforms, speaker_ids, lengths = next(iter(dev_loader))\n",
    "    waveforms, speaker_ids, lengths = waveforms.to(device), speaker_ids.to(device), lengths.to(device)\n",
    "    \n",
    "    # 1. 원본 \"임베딩\" (공격의 목표점)\n",
    "    with torch.no_grad():\n",
    "        rel_lengths = lengths.float() / waveforms.shape[1]\n",
    "        original_embeddings = speaker_model.encode_batch(waveforms, wav_lens=rel_lengths).squeeze(1)\n",
    "\n",
    "    # 2. 원본 \"멜 스펙트로그램\" (공격의 시작점)\n",
    "    with torch.no_grad():\n",
    "        wav_input = waveforms.squeeze(0) # [B, T_wav] -> [T_wav]\n",
    "        \n",
    "        # SpeechBrain 함수 호출 (출력: [Mels, T] 즉 [80, 306])\n",
    "        original_mels, _ = mel_spectogram(\n",
    "            audio=wav_input,\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            win_length=WIN_LENGTH,\n",
    "            n_mels=N_MELS,\n",
    "            n_fft=N_FFT,\n",
    "            f_min=F_MIN,\n",
    "            f_max=F_MAX,\n",
    "            power=POWER,\n",
    "            normalized=False,\n",
    "            min_max_energy_norm=True,\n",
    "            norm=\"slaney\",\n",
    "            mel_scale=\"slaney\",\n",
    "            compression=COMPRESSION\n",
    "        )\n",
    "        \n",
    "        # [!!! --- 핵심 수정 --- !!!]\n",
    "        # .transpose(0, 1)를 제거하고 .unsqueeze(0)만 사용하여 배치 차원 추가\n",
    "        # [Mels, T] -> [B, Mels, T]\n",
    "        original_mels = original_mels.unsqueeze(0) \n",
    "        # (이전 코드: original_mels = original_mels.transpose(0, 1).unsqueeze(0))\n",
    "        \n",
    "    print(f\"배치 로드 완료. 파형 Shape: {waveforms.shape}\")\n",
    "    print(f\"원본 임베딩(타겟) Shape: {original_embeddings.shape}\")\n",
    "    print(f\"원본 멜(공격대상) Shape: {original_mels.shape}\") # [1, 80, 306]이 출력되어야 함\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"[치명적 오류] 3단계 오류: 데이터로더가 비어있습니다. (StopIteration)\")\n",
    "    raise SystemExit(\"데이터 로드 실패.\")\n",
    "except Exception as e:\n",
    "    print(f\"[치명적 오류] 3단계 데이터 처리 중 오류 발생: {e}\")\n",
    "    raise SystemExit(\"데이터 처리 실패.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3.5단계: 베이스라인 검증 시작...\n",
      "베이스라인 검증 완료. '0_baseline_vocoder.wav'를 다시 확인하세요.\n",
      ">>> '0_original_raw.wav'와 소리가 거의 동일해야 합니다.\n",
      "\n",
      "--- 2-3/3/3.5단계 (준비) 완료. 4단계(PGD 공격)를 진행할 수 있습니다. ---\n"
     ]
    }
   ],
   "source": [
    "# --- 3.5. [필수] 베이스라인 검증 ---\n",
    "print(\"\\n3.5단계: 베이스라인 검증 시작...\")\n",
    "output_dir = \"attack_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # [수정] 이제 original_mels는 올바른 [1, 80, 306] Shape을 가짐\n",
    "        baseline_waveform = vocoder.mods.generator(original_mels).squeeze(1).cpu()\n",
    "\n",
    "    # (이하 파일 저장 코드 동일)\n",
    "    torchaudio.save(\n",
    "        os.path.join(output_dir, \"ref/0_original_raw.wav\"), \n",
    "        waveforms[0].cpu().unsqueeze(0), \n",
    "        SAMPLE_RATE\n",
    "    )\n",
    "    torchaudio.save(\n",
    "        os.path.join(output_dir, \"output/0_baseline_vocoder.wav\"), \n",
    "        baseline_waveform[0].unsqueeze(0), \n",
    "        SAMPLE_RATE\n",
    "    )\n",
    "    print(\"베이스라인 검증 완료. '0_baseline_vocoder.wav'를 다시 확인하세요.\")\n",
    "    print(\">>> '0_original_raw.wav'와 소리가 거의 동일해야 합니다.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[치명적 오류] 3.5단계 베이스라인 검증 중 오류: {e}\")\n",
    "    raise SystemExit(\"베이스라인 검증 실패.\")\n",
    "\n",
    "print(\"\\n--- 2-3/3/3.5단계 (준비) 완료. 4단계(PGD 공격)를 진행할 수 있습니다. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4단계: PGD 공격 시작 (Target: Standard Mel) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PGD Attack Loop [Loss: -760.39324951]: 100%|██████████| 40/40 [00:01<00:00, 27.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PGD 공격 완료 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. PGD 공격 구현 (수정된 파이프라인) ---\n",
    "print(\"\\n--- 4단계: PGD 공격 시작 (Target: Standard Mel) ---\")\n",
    "\n",
    "# PGD 하이퍼파라미터 (로그-멜 스펙트로그램 기준이므로 epsilon을 작게 시작)\n",
    "epsilon = 0.05   # [조정 필요] 최대 노이즈 크기\n",
    "learning_rate = 0.005 # [조정 필요] 스텝 크기\n",
    "iterations = 40    # 반복 횟수\n",
    "\n",
    "# 1. 노이즈 텐서(delta) 랜덤 초기화 (original_mels와 동일한 크기)\n",
    "#    (epsilon 범위 내의 랜덤 값으로 시작)\n",
    "delta_init = (torch.rand_like(original_mels) * 2 - 1) * epsilon\n",
    "delta = delta_init.to(device)\n",
    "delta.requires_grad = True # 이 텐서의 그래디언트를 계산\n",
    "\n",
    "# 2. 손실 함수 (MSE Loss)\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "pbar = tqdm(range(iterations), desc=\"PGD Attack Loop\")\n",
    "\n",
    "for i in pbar: \n",
    "    try:\n",
    "        # 3. 계산 그래프 생성 강제\n",
    "        with torch.enable_grad(): \n",
    "            # 4. 적대적 멜 스펙트로그램 생성\n",
    "            adv_mels = original_mels + delta\n",
    "            \n",
    "            # [수정된 파이프라인]\n",
    "            # 5. 멜 -> 파형 (Differentiable Vocoder)\n",
    "            #    (입력 Shape: [B, N_Mels, T], HiFi-GAN G 입력과 일치)\n",
    "            adv_waveform = vocoder.mods.generator(adv_mels).squeeze(1) \n",
    "            \n",
    "            # 6. 생성된 파형의 '새로운 상대 길이' 계산\n",
    "            #    (현재 batch_size=1이므로 길이는 항상 1.0)\n",
    "            if adv_waveform.shape[0] == 0 or adv_waveform.shape[1] == 0:\n",
    "                tqdm.write(\"!!오류: 보코더가 빈 오디오를 생성했습니다.\")\n",
    "                continue\n",
    "            \n",
    "            # (ECAPA-TDNN의 encode_batch는 wav_lens를, \n",
    "            #  내부 embedding_model은 상대 길이를 받음)\n",
    "            adv_rel_lengths = torch.tensor([1.0] * adv_waveform.shape[0], device=device)\n",
    "\n",
    "            # 7. 파형 -> 화자 인코더 피처 (Feature Extractor)\n",
    "            adv_features_re_extracted = feature_extractor(adv_waveform)\n",
    "            \n",
    "            # 8. 피처 -> 임베딩 (Embedding Model)\n",
    "            adv_embeddings = embedding_model(adv_features_re_extracted, adv_rel_lengths).squeeze(1)\n",
    "            \n",
    "            # 9. 손실 계산 (원본 임베딩과의 거리를 최대화)\n",
    "            loss = -mse_loss(adv_embeddings, original_embeddings.detach())\n",
    "        \n",
    "        # 10. 그래디언트 계산\n",
    "        speaker_model.zero_grad()\n",
    "        vocoder.zero_grad() \n",
    "        loss.backward() \n",
    "\n",
    "        # 11. PGD 업데이트 (Loss가 음수이므로 -)\n",
    "        delta.data = delta.data - learning_rate * delta.grad.sign()\n",
    "        \n",
    "        # 12. 프로젝션 (L-infinity): delta가 epsilon 예산을 넘지 않도록 제한\n",
    "        delta.data = torch.clamp(delta.data, -epsilon, epsilon)\n",
    "        \n",
    "        # 13. delta 그래디언트 초기화\n",
    "        delta.grad.zero_()\n",
    "\n",
    "        pbar.set_description(f\"PGD Attack Loop [Loss: {loss.item():.8f}]\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        tqdm.write(f\"\\n[오류 발생] Iteration {i+1}: PGD 루프 실패.\")\n",
    "        tqdm.write(f\"오류 메시지: {e}\")\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            tqdm.write(\">>> [제안] CUDA OutOfMemory. batch_size=1인지 확인하세요.\")\n",
    "        break \n",
    "\n",
    "pbar.close()\n",
    "print(\"--- PGD 공격 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5단계: 공격 검증 시작 ---\n",
      "\n",
      "--- 공격 검증 결과 (Domain-Aligned) ---\n",
      "원본 vs 적대적 임베딩 간 코사인 유사도 (1.0 = 동일, 낮을수록 성공):\n",
      "  -> 0.5193\n",
      "참고: 원본 vs 다른 화자 간 평균 유사도 (베이스라인): 1.0000\n",
      "\n",
      "결과 오디오 파일 '1_adversarial_attack.wav'가 'attack_results' 폴더에 저장되었습니다.\n",
      "\n",
      ">>> [최종 검증] '0_baseline_vocoder.wav'와 '1_adversarial_attack.wav'를 비교 청취하세요.\n",
      ">>> 1. 두 파일의 음질이 유사합니까? (비인지성 성공)\n",
      ">>> 2. 코사인 유사도 값이 (e.g., 0.9) 베이스라인(e.g., 0.2)에 가깝게 떨어졌습니까? (공격 성공)\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 공격 성공 여부 검증 ---\n",
    "print(\"\\n--- 5단계: 공격 검증 시작 ---\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # 1. 최종 적대적 멜 생성\n",
    "        final_adv_mels = (original_mels + delta).detach()\n",
    "        \n",
    "        # 2. 최종 적대적 '오디오 파형' 생성 (비인지성 검증 대상!)\n",
    "        final_adv_waveform = vocoder.mods.generator(final_adv_mels).squeeze(1).cpu()\n",
    "\n",
    "        # 3. 최종 적대적 '임베딩' 계산 (공격 성공률 검증)\n",
    "        #    (PGD 루프와 동일한 과정을 no_grad로 수행)\n",
    "        adv_wav_gpu = final_adv_waveform.to(device)\n",
    "        adv_rel_lengths = torch.tensor([1.0] * adv_wav_gpu.shape[0], device=device)\n",
    "        adv_features_re = feature_extractor(adv_wav_gpu)\n",
    "        final_adv_embeddings = embedding_model(adv_features_re, adv_rel_lengths).squeeze(1).cpu()\n",
    "\n",
    "    # 코사인 유사도 검증\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(\n",
    "        original_embeddings.cpu(), \n",
    "        final_adv_embeddings, \n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- 공격 검증 결과 (Domain-Aligned) ---\")\n",
    "    print(f\"원본 vs 적대적 임베딩 간 코사인 유사도 (1.0 = 동일, 낮을수록 성공):\")\n",
    "    print(f\"  -> {cos_sim.item():.4f}\") # (batch_size=1이므로 .mean() 대신 .item())\n",
    "\n",
    "    # 베이스라인 비교 (다른 화자와의 유사도)\n",
    "    shuffled_orig_embeddings = original_embeddings[torch.randperm(original_embeddings.size(0))]\n",
    "    baseline_cos_sim = torch.nn.functional.cosine_similarity(\n",
    "        original_embeddings.cpu(), \n",
    "        shuffled_orig_embeddings.cpu(),\n",
    "        dim=1\n",
    "    )\n",
    "    print(f\"참고: 원본 vs 다른 화자 간 평균 유사도 (베이스라인): {baseline_cos_sim.mean().item():.4f}\")\n",
    "\n",
    "\n",
    "    # 오디오 파일 저장 (비교 청취용)\n",
    "    torchaudio.save(\n",
    "        os.path.join(output_dir, \"attack/1_adversarial_attack.wav\"), \n",
    "        final_adv_waveform[0].unsqueeze(0), \n",
    "        SAMPLE_RATE\n",
    "    )\n",
    "    print(f\"\\n결과 오디오 파일 '1_adversarial_attack.wav'가 '{output_dir}' 폴더에 저장되었습니다.\")\n",
    "    print(\"\\n>>> [최종 검증] '0_baseline_vocoder.wav'와 '1_adversarial_attack.wav'를 비교 청취하세요.\")\n",
    "    print(\">>> 1. 두 파일의 음질이 유사합니까? (비인지성 성공)\")\n",
    "    print(\">>> 2. 코사인 유사도 값이 (e.g., 0.9) 베이스라인(e.g., 0.2)에 가깝게 떨어졌습니까? (공격 성공)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[치명적 오류] 5단계 검증 중 오류: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5단계: 일반화 검증 시작 ---\n",
      "총 40개의 샘플에 대해 일반화 검증을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generalization Test [Avg CosSim: 0.4619]: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40개 샘플 테스트 완료. 루프를 중단합니다.\n",
      "\n",
      "--- 일반화 검증 완료 ---\n",
      "테스트한 총 샘플 수: 40\n",
      "평균 코사인 유사도: 0.4619\n",
      "최저 유사도 (가장 공격 성공): 0.0992\n",
      "최고 유사도 (가장 공격 실패): 0.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torchaudio\n",
    "\n",
    "print(\"\\n--- 5단계: 일반화 검증 시작 ---\")\n",
    "\n",
    "# --- 테스트 파라미터 ---\n",
    "max_samples_to_test = 40  # [조정 가능] 검증할 총 샘플 수 (전체는 수천 개)\n",
    "pgd_iterations = 40       # PGD 반복 횟수\n",
    "pgd_epsilon = 0.05        # 멜 노이즈 크기 (이전과 동일)\n",
    "pgd_learning_rate = 0.005 # PGD 스텝 크기 (이전과 동일)\n",
    "\n",
    "# --- PGD 공격에 필요한 파라미터 (이전 단계에서 복사) ---\n",
    "SAMPLE_RATE = 16000\n",
    "HOP_LENGTH = 256\n",
    "# (mel_spectogram 함수가 이미 정의되어 있다고 가정)\n",
    "\n",
    "# --- 결과 저장용 리스트 ---\n",
    "all_cosine_similarities = []\n",
    "\n",
    "# --- 데이터로더 순회 ---\n",
    "# (이전 단계에서 정의된 dev_loader 사용)\n",
    "print(f\"총 {max_samples_to_test}개의 샘플에 대해 일반화 검증을 시작합니다...\")\n",
    "pbar_loader = tqdm(dev_loader, total=max_samples_to_test, desc=\"Generalization Test\")\n",
    "\n",
    "for i, batch in enumerate(pbar_loader):\n",
    "    if i >= max_samples_to_test:\n",
    "        tqdm.write(f\"{max_samples_to_test}개 샘플 테스트 완료. 루프를 중단합니다.\")\n",
    "        break  # 테스트 샘플 수에 도달하면 중지\n",
    "\n",
    "    try:\n",
    "        # --- 3단계 (적응): 원본 데이터 및 기준점 추출 ---\n",
    "        pbar_loader.set_description(f\"Sample {i+1} [Step 3: Loading data]\")\n",
    "        \n",
    "        waveforms, speaker_ids, lengths = batch\n",
    "        waveforms, speaker_ids, lengths = waveforms.to(device), speaker_ids.to(device), lengths.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 1. 원본 임베딩 (공격 타겟)\n",
    "            rel_lengths = lengths.float() / waveforms.shape[1]\n",
    "            original_embeddings = speaker_model.encode_batch(waveforms, wav_lens=rel_lengths).squeeze(1)\n",
    "\n",
    "            # 2. 원본 멜 스펙트로그램 (공격 대상)\n",
    "            wav_input = waveforms.squeeze(0)\n",
    "            original_mels, _ = mel_spectogram(\n",
    "                audio=wav_input, sample_rate=SAMPLE_RATE, hop_length=HOP_LENGTH,\n",
    "                win_length=1024, n_mels=80, n_fft=1024, f_min=0.0, f_max=8000.0,\n",
    "                power=1.0, normalized=False, min_max_energy_norm=True,\n",
    "                norm=\"slaney\", mel_scale=\"slaney\", compression=True\n",
    "            )\n",
    "            original_mels = original_mels.unsqueeze(0) # [1, Mels, T]\n",
    "\n",
    "        # --- 4단계 (적응): PGD 공격 루프 ---\n",
    "        pbar_loader.set_description(f\"Sample {i+1} [Step 4: PGD Attack]\")\n",
    "\n",
    "        delta_init = (torch.rand_like(original_mels) * 2 - 1) * pgd_epsilon\n",
    "        delta = delta_init.to(device)\n",
    "        delta.requires_grad = True\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "        # (내부 PGD 루프는 tqdm을 표시하지 않음)\n",
    "        for _ in range(pgd_iterations):\n",
    "            with torch.enable_grad():\n",
    "                adv_mels = original_mels + delta\n",
    "                adv_waveform = vocoder.mods.generator(adv_mels).squeeze(1)\n",
    "                \n",
    "                if adv_waveform.shape[0] == 0 or adv_waveform.shape[1] == 0:\n",
    "                    continue # 빈 오디오 생성 시 스킵\n",
    "                \n",
    "                adv_rel_lengths = torch.tensor([1.0] * adv_waveform.shape[0], device=device)\n",
    "                adv_features_re = feature_extractor(adv_waveform)\n",
    "                adv_embeddings = embedding_model(adv_features_re, adv_rel_lengths).squeeze(1)\n",
    "                loss = -mse_loss(adv_embeddings, original_embeddings.detach())\n",
    "            \n",
    "            speaker_model.zero_grad()\n",
    "            vocoder.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            delta.data = delta.data - pgd_learning_rate * delta.grad.sign()\n",
    "            delta.data = torch.clamp(delta.data, -pgd_epsilon, pgd_epsilon)\n",
    "            delta.grad.zero_()\n",
    "\n",
    "        # --- 5단계 (적응): 최종 검증 ---\n",
    "        pbar_loader.set_description(f\"Sample {i+1} [Step 5: Verifying]\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            final_adv_mels = (original_mels + delta).detach()\n",
    "            final_adv_waveform = vocoder.mods.generator(final_adv_mels).squeeze(1).cpu()\n",
    "            \n",
    "            adv_wav_gpu = final_adv_waveform.to(device)\n",
    "            adv_rel_lengths_final = torch.tensor([1.0] * adv_wav_gpu.shape[0], device=device)\n",
    "            adv_features_re_final = feature_extractor(adv_wav_gpu)\n",
    "            final_adv_embeddings = embedding_model(adv_features_re_final, adv_rel_lengths_final).squeeze(1).cpu()\n",
    "\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(\n",
    "                original_embeddings.cpu(), \n",
    "                final_adv_embeddings, \n",
    "                dim=1\n",
    "            )\n",
    "            \n",
    "            all_cosine_similarities.append(cos_sim.item())\n",
    "\n",
    "        # (선택 사항) 매 N번째 샘플마다 공격 오디오 저장\n",
    "        if i < 5: # 처음 5개 샘플만 저장\n",
    "            torchaudio.save(\n",
    "                os.path.join(output_dir, f\"{i}_adversarial_attack.wav\"), \n",
    "                final_adv_waveform[0].unsqueeze(0), \n",
    "                SAMPLE_RATE\n",
    "            )\n",
    "            \n",
    "        # Tqdm 진행률 표시줄 업데이트\n",
    "        pbar_loader.set_description(f\"Generalization Test [Avg CosSim: {np.mean(all_cosine_similarities):.4f}]\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        tqdm.write(f\"\\n[오류 발생] Sample {i+1} 처리 중 오류: {e}\")\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            tqdm.write(\">>> CUDA OutOfMemory. 오디오가 너무 길 수 있습니다. 다음 샘플로 넘어갑니다.\")\n",
    "            continue # OOM 발생 시 다음 샘플로\n",
    "        else:\n",
    "            break # 다른 오류 시 중단\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"\\n[알 수 없는 오류] Sample {i+1} 처리 중 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "pbar_loader.close()\n",
    "\n",
    "# --- 최종 결과 리포트 ---\n",
    "print(\"\\n--- 일반화 검증 완료 ---\")\n",
    "if all_cosine_similarities:\n",
    "    avg_sim = np.mean(all_cosine_similarities)\n",
    "    min_sim = np.min(all_cosine_similarities)\n",
    "    max_sim = np.max(all_cosine_similarities)\n",
    "    \n",
    "    print(f\"테스트한 총 샘플 수: {len(all_cosine_similarities)}\")\n",
    "    print(f\"평균 코사인 유사도: {avg_sim:.4f}\")\n",
    "    print(f\"최저 유사도 (가장 공격 성공): {min_sim:.4f}\")\n",
    "    print(f\"최고 유사도 (가장 공격 실패): {max_sim:.4f}\")\n",
    "else:\n",
    "    print(\"검증이 완료된 샘플이 없습니다. 오류 로그를 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
