{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "C:\\Users\\mmc\\AppData\\Local\\Temp\\ipykernel_43884\\4110274581.py:12: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import random\n",
    "\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIBRISPEECH_ROOT = c:\\project\\vcshield\\data\\train\\dev-clean\n"
     ]
    }
   ],
   "source": [
    "#config\n",
    "####################################\n",
    "# 경로 및 공통 설정\n",
    "####################################\n",
    "\n",
    "# 실험 노트북(.ipynb)이 있는 위치를 기준으로 상대경로를 잡는다고 가정\n",
    "PROJECT_ROOT = os.path.abspath(\".\")  # 필요하면 직접 바꿔도 됨\n",
    "\n",
    "LIBRISPEECH_ROOT = os.path.join(PROJECT_ROOT, \"data\", \"train\", \"dev-clean\")\n",
    "# 예: ./data/train/dev-clean/84/..., 174/..., SPEAKERS.TXT ...\n",
    "\n",
    "print(\"LIBRISPEECH_ROOT =\", LIBRISPEECH_ROOT)\n",
    "assert os.path.isdir(LIBRISPEECH_ROOT), \"dev-clean 경로를 확인하세요.\"\n",
    "\n",
    "# 오디오 파라미터\n",
    "SAMPLE_RATE = 16000  # LibriSpeech 기본 16kHz\n",
    "\n",
    "# DataLoader 파라미터\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0  # 윈도우면 0~2 정도로, 리눅스면 더 올려도 됨\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "\n",
    "# 특정 화자만 사용할 경우 지정 (None이면 전체 화자)\n",
    "# 예: speaker_list = [\"84\", \"174\"]\n",
    "speaker_list = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibriSpeechSpeakerDataset] speakers: 40, utterances(raw): 2703\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "class LibriSpeechSpeakerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    LibriSpeech dev-clean에서 화자 단위로 발화들을 로드한다.\n",
    "    각 item은 dict:\n",
    "        {\n",
    "          \"waveform\": Tensor shape (1, T) float32 [-1,1],\n",
    "          \"speaker_id\": str,\n",
    "          \"utt_path\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        target_sr: int = 16000,\n",
    "        speaker_list: Optional[List[str]] = None,\n",
    "        min_duration_sec: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        root_dir: dev-clean 경로\n",
    "        target_sr: 리샘플할 샘플레이트 (보통 16k)\n",
    "        speaker_list: 사용할 화자 ID 리스트. None이면 root_dir 내 모든 화자 ID 사용.\n",
    "        min_duration_sec: 너무 짧은 음성을 버리기 위한 최소 길이(초)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.target_sr = target_sr\n",
    "        self.min_duration_sec = min_duration_sec\n",
    "\n",
    "        # 1) 화자 폴더 수집 (폴더명이 전부 숫자인 것만 화자로 간주)\n",
    "        all_speakers = [\n",
    "            d for d in os.listdir(root_dir)\n",
    "            if os.path.isdir(os.path.join(root_dir, d)) and d.isdigit()\n",
    "        ]\n",
    "\n",
    "        if speaker_list is None:\n",
    "            self.speakers = sorted(all_speakers)\n",
    "        else:\n",
    "            # 교집합만 취함\n",
    "            self.speakers = sorted([s for s in all_speakers if s in speaker_list])\n",
    "\n",
    "        # 2) 각 화자에서 실제 오디오 파일(.flac/.wav) 경로 모으기\n",
    "        #    LibriSpeech는 일반적으로 spk_id/chapter_id/*.flac 형태\n",
    "        self.items = []\n",
    "        for spk in self.speakers:\n",
    "            spk_dir = os.path.join(root_dir, spk)\n",
    "            # 챕터 디렉토리들\n",
    "            for ch_name in os.listdir(spk_dir):\n",
    "                ch_dir = os.path.join(spk_dir, ch_name)\n",
    "                if not os.path.isdir(ch_dir):\n",
    "                    continue\n",
    "\n",
    "                # .wav / .flac 다 지원\n",
    "                wav_paths = glob.glob(os.path.join(ch_dir, \"*.wav\"))\n",
    "                flac_paths = glob.glob(os.path.join(ch_dir, \"*.flac\"))\n",
    "                audio_paths = sorted(wav_paths + flac_paths)\n",
    "\n",
    "                for ap in audio_paths:\n",
    "                    # 길이 필터링을 위해 일단 메타만 등록하고,\n",
    "                    # 실제 __getitem__에서 필요하면 필터하자.\n",
    "                    self.items.append({\n",
    "                        \"speaker_id\": spk,\n",
    "                        \"utt_path\": ap\n",
    "                    })\n",
    "\n",
    "        print(f\"[LibriSpeechSpeakerDataset] speakers: {len(self.speakers)}, utterances(raw): {len(self.items)}\")\n",
    "\n",
    "        # 사전 길이 필터를 적용해도 되지만, 여기서는 __getitem__에서 처리 실패 시 재시도 하기보단\n",
    "        # 그냥 __len__/__getitem__이 일관되게 동작하도록 그대로 둔다.\n",
    "        # (필요하면 나중에 pre-filter 로직 추가 가능)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def _load_audio(self, path: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        path에서 오디오 로딩하고 mono+resample까지 맞춰서 (1, T) float32 [-1,1] 반환\n",
    "        \"\"\"\n",
    "        wav, sr = torchaudio.load(path)  # wav: (C, T), float32 -1~1 범위일 가능성 높음\n",
    "        # 모노화\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "        # 리샘플\n",
    "        if sr != self.target_sr:\n",
    "            wav = torchaudio.functional.resample(wav, sr, self.target_sr)\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        meta = self.items[idx]\n",
    "        spk = meta[\"speaker_id\"]\n",
    "        path = meta[\"utt_path\"]\n",
    "\n",
    "        wav = self._load_audio(path)  # (1, T)\n",
    "\n",
    "        # 너무 짧으면(말 없는 부분 등) downstream에서 학습이 불안정할 수 있으므로 여기서 잘라낼 수 있다.\n",
    "        # 여기서는 min_duration_sec 이상만 보장하도록 잘라주는 정도만 (필요하다면)\n",
    "        min_len = int(self.min_duration_sec * self.target_sr)\n",
    "        if wav.shape[1] < min_len:\n",
    "            # 너무 짧다면 패딩 혹은 스킵 로직을 짤 수도 있다.\n",
    "            # 간단하게는 zero-pad\n",
    "            pad_len = min_len - wav.shape[1]\n",
    "            wav = torch.cat([wav, torch.zeros((1, pad_len), dtype=wav.dtype)], dim=1)\n",
    "\n",
    "        return {\n",
    "            \"waveform\": wav,       # (1, T_resampled)\n",
    "            \"speaker_id\": spk,     # string\n",
    "            \"utt_path\": path       # string\n",
    "        }\n",
    "    \n",
    "dataset = LibriSpeechSpeakerDataset(\n",
    "    root_dir=LIBRISPEECH_ROOT,\n",
    "    target_sr=SAMPLE_RATE,\n",
    "    speaker_list=speaker_list,   # None이면 전체 speaker\n",
    "    min_duration_sec=0.5         # 너무 짧은 샘플은 최소 0.5초 길이로 패딩\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from speechbrain.utils.fetching import LocalStrategy\n",
    "\n",
    "class ECAPASpeakerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ECAPA-TDNN speaker encoder wrapper.\n",
    "    - freeze params (no finetune)\n",
    "    - BUT allow gradients to flow w.r.t. the input waveform\n",
    "      so L_resist can push G through vocoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.classifier = EncoderClassifier.from_hparams(\n",
    "            source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "            run_opts={\"device\": device},\n",
    "            savedir=\"./pretrained_ecapa\",\n",
    "            local_strategy=LocalStrategy.COPY,  # <= Windows safe (copies instead of symlink) :contentReference[oaicite:1]{index=1}\n",
    "        )\n",
    "\n",
    "        # freeze ECAPA weights so we don't fine-tune it\n",
    "        for p in self.classifier.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, wave_batch):\n",
    "        \"\"\"\n",
    "        wave_batch: (B,1,T) float32 @16kHz\n",
    "        returns: (B, emb_dim)\n",
    "        NOTE: no torch.no_grad() here. We WANT grad to flow back\n",
    "              from cosine loss into wave_batch (→ vocoder → G).\n",
    "        \"\"\"\n",
    "        if wave_batch.dim() == 3 and wave_batch.shape[1] == 1:\n",
    "            wave_in = wave_batch.squeeze(1)  # (B,T)\n",
    "        else:\n",
    "            wave_in = wave_batch            # assume already (B,T)\n",
    "\n",
    "        # encode_batch returns (B,1,emb_dim)\n",
    "        emb = self.classifier.encode_batch(wave_in)\n",
    "        emb = emb.squeeze(1)               # (B, emb_dim)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sanity Check w/ collate_fn ===\n",
      "waveform.shape: torch.Size([4, 1, 116800])\n",
      "speaker_id example: ['1919', '652', '2277', '6295']\n",
      "utt_path[0]: c:\\project\\vcshield\\data\\train\\dev-clean\\1919\\142785\\1919-142785-0028.flac\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def match_spatial(src, ref):\n",
    "    \"\"\"\n",
    "    src, ref: 4D 텐서 (B,C,H,W)\n",
    "    리턴: src와 ref 중 작은 쪽 크기로 둘을 맞춰서 (src_adj, ref_adj)를 돌려준다.\n",
    "    - 만약 H/W가 서로 다르면\n",
    "      중앙을 기준으로 잘라서 동일한 (H',W')로 맞춤\n",
    "    - src, ref 중 어느 쪽이 더 큰지/작은지는 각각 축마다 독립적으로 처리\n",
    "    \"\"\"\n",
    "    B1,C1,Hs,Ws = src.shape\n",
    "    B2,C2,Hr,Wr = ref.shape\n",
    "    assert B1 == B2, \"batch mismatch in match_spatial\"\n",
    "\n",
    "    # 최종 목표 크기\n",
    "    Ht = min(Hs, Hr)\n",
    "    Wt = min(Ws, Wr)\n",
    "\n",
    "    def center_crop(t, Ht, Wt):\n",
    "        _,_,H,W = t.shape\n",
    "        start_h = (H - Ht)//2\n",
    "        start_w = (W - Wt)//2\n",
    "        return t[:,:,start_h:start_h+Ht, start_w:start_w+Wt]\n",
    "\n",
    "    src_c = center_crop(src, Ht, Wt)\n",
    "    ref_c = center_crop(ref, Ht, Wt)\n",
    "    return src_c, ref_c\n",
    "\n",
    "def collate_with_padding(batch_list):\n",
    "    \"\"\"\n",
    "    batch_list는 __getitem__에서 나온 dict들의 리스트 (len = B)\n",
    "\n",
    "    목표:\n",
    "    - waveform들을 가장 긴 샘플 길이에 맞춰 zero-pad (right padding)\n",
    "    - speaker_id / utt_path는 리스트 그대로 유지\n",
    "    \"\"\"\n",
    "    wave_list = [item[\"waveform\"] for item in batch_list]  # [(1, T_i), ...]\n",
    "    spk_list = [item[\"speaker_id\"] for item in batch_list]\n",
    "    path_list = [item[\"utt_path\"] for item in batch_list]\n",
    "\n",
    "    # 가장 긴 길이 구하기\n",
    "    max_len = max(w.shape[1] for w in wave_list)\n",
    "\n",
    "    # pad 후 스택\n",
    "    padded = []\n",
    "    for w in wave_list:\n",
    "        if w.shape[1] < max_len:\n",
    "            pad_amount = max_len - w.shape[1]\n",
    "            w = F.pad(w, (0, pad_amount), mode=\"constant\", value=0.0)\n",
    "        padded.append(w)  # (1, max_len)\n",
    "    wave_tensor = torch.stack(padded, dim=0)  # (B, 1, max_len)\n",
    "\n",
    "    return {\n",
    "        \"waveform\": wave_tensor,\n",
    "        \"speaker_id\": spk_list,\n",
    "        \"utt_path\": path_list\n",
    "    }\n",
    "\n",
    "# collate_fn을 적용한 DataLoader\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_with_padding\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(\"=== Sanity Check w/ collate_fn ===\")\n",
    "print(\"waveform.shape:\", batch[\"waveform\"].shape)  # (B, 1, max_len_in_batch)\n",
    "print(\"speaker_id example:\", batch[\"speaker_id\"])\n",
    "print(\"utt_path[0]:\", batch[\"utt_path\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveform: torch.Size([4, 1, 281120])\n",
      "mel_db: torch.Size([4, 80, 1099])\n",
      "speaker_id: ['2086', '251', '1272', '5895']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def build_mel_extractor(\n",
    "    sample_rate=16000,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_mels=80,\n",
    "    f_min=0.0,\n",
    "    f_max=8000.0,\n",
    "    power=1.0,\n",
    "    log_offset=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    반환 함수 wav_to_mel_db:\n",
    "      입력  : waveform (B,1,T) 또는 (1,T)\n",
    "      출력  : mel_batch (B, n_mels, time)\n",
    "    \"\"\"\n",
    "\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        n_mels=n_mels,\n",
    "        power=power,\n",
    "    )\n",
    "\n",
    "    def wav_to_mel_db(waveform: torch.Tensor):\n",
    "        \"\"\"\n",
    "        waveform: (B,1,T) or (1,T)\n",
    "        returns: (B, n_mels, time)\n",
    "        \"\"\"\n",
    "        single_input = False\n",
    "\n",
    "        # case: (1,T) -> (1,1,T)\n",
    "        if waveform.dim() == 2:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "            single_input = True\n",
    "\n",
    "        # sanity check\n",
    "        if waveform.dim() != 3 or waveform.shape[1] != 1:\n",
    "            raise ValueError(f\"Expected (B,1,T) or (1,T); got {tuple(waveform.shape)}\")\n",
    "\n",
    "        B = waveform.size(0)\n",
    "        mel_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            # waveform[b]: (1, T)\n",
    "            mel = mel_transform(waveform[b])  # (1, n_mels, time)\n",
    "            # squeeze channel dim -> (n_mels, time)\n",
    "            mel = mel.squeeze(0)\n",
    "\n",
    "            mel_db = torch.log(mel + log_offset)  # (n_mels, time)\n",
    "            mel_list.append(mel_db)\n",
    "\n",
    "        # stack -> (B, n_mels, time)\n",
    "        mel_batch = torch.stack(mel_list, dim=0)\n",
    "\n",
    "        if single_input:\n",
    "            mel_batch = mel_batch[0]  # (n_mels, time)\n",
    "\n",
    "        return mel_batch\n",
    "\n",
    "    return wav_to_mel_db\n",
    "\n",
    "\n",
    "# 빌드\n",
    "wav_to_mel_db = build_mel_extractor(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_mels=80,\n",
    "    f_min=0.0,\n",
    "    f_max=8000.0,\n",
    "    power=1.0,\n",
    "    log_offset=1e-6\n",
    ")\n",
    "\n",
    "# 테스트\n",
    "test_batch = next(iter(loader))\n",
    "test_wave = test_batch[\"waveform\"]      # (B,1,T)\n",
    "test_mel  = wav_to_mel_db(test_wave)    # 기대: (B,80,time)\n",
    "\n",
    "print(\"waveform:\", test_wave.shape)\n",
    "print(\"mel_db:\", test_mel.shape)\n",
    "print(\"speaker_id:\", test_batch[\"speaker_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class StochasticPurifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 time_blur_kernel_sizes=(3,5,7),\n",
    "                 freq_blur_kernel_sizes=(3,5),\n",
    "                 downsample_factors=(2,3)):\n",
    "        super().__init__()\n",
    "        self.time_blur_kernel_sizes = time_blur_kernel_sizes\n",
    "        self.freq_blur_kernel_sizes = freq_blur_kernel_sizes\n",
    "        self.downsample_factors = downsample_factors\n",
    "\n",
    "    def _time_blur(self, mel):\n",
    "        B, Fm, Tm = mel.shape\n",
    "        k = random.choice(self.time_blur_kernel_sizes)\n",
    "        pad = k // 2\n",
    "        kernel = torch.ones(Fm, 1, k, device=mel.device, dtype=mel.dtype) / k\n",
    "        mel_pad = F.pad(mel, (pad, pad), mode='reflect')\n",
    "        out = F.conv1d(mel_pad, kernel, stride=1, padding=0, groups=Fm)\n",
    "        return out\n",
    "\n",
    "    def _time_down_up(self, mel):\n",
    "        B, Fm, Tm = mel.shape\n",
    "        factor = random.choice(self.downsample_factors)\n",
    "        if Tm // factor < 2:\n",
    "            return mel\n",
    "        T_down = max(2, Tm // factor)\n",
    "        mel_down = F.interpolate(\n",
    "            mel.unsqueeze(1),  # (B,1,Fm,Tm)\n",
    "            size=(Fm, T_down),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze(1)  # (B,Fm,T_down)\n",
    "        mel_up = F.interpolate(\n",
    "            mel_down.unsqueeze(1),\n",
    "            size=(Fm, Tm),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze(1)  # (B,Fm,Tm)\n",
    "        return mel_up\n",
    "\n",
    "    def _freq_blur(self, mel):\n",
    "        B, Fm, Tm = mel.shape\n",
    "        k = random.choice(self.freq_blur_kernel_sizes)\n",
    "        pad = k // 2\n",
    "\n",
    "        mel_t = mel.transpose(1,2)      # (B,Tm,Fm)\n",
    "        mel_bt = mel_t.reshape(B*Tm,1,Fm)\n",
    "        kernel = torch.ones(1,1,k, device=mel.device, dtype=mel.dtype)/k\n",
    "        mel_bt_pad = F.pad(mel_bt, (pad,pad), mode='reflect')\n",
    "        mel_bt_blur = F.conv1d(mel_bt_pad, kernel, stride=1, padding=0)\n",
    "        mel_blur_t = mel_bt_blur.reshape(B,Tm,Fm)\n",
    "        mel_blur = mel_blur_t.transpose(1,2)  # (B,Fm,Tm)\n",
    "        return mel_blur\n",
    "\n",
    "    def forward(self, mel):\n",
    "        ops = []\n",
    "        if random.random() < 0.7:\n",
    "            ops.append(\"time_blur\")\n",
    "        if random.random() < 0.5:\n",
    "            ops.append(\"time_down_up\")\n",
    "        if random.random() < 0.5:\n",
    "            ops.append(\"freq_blur\")\n",
    "\n",
    "        x = mel\n",
    "        for o in ops:\n",
    "            if o == \"time_blur\":\n",
    "                x = self._time_blur(x)\n",
    "            elif o == \"time_down_up\":\n",
    "                x = self._time_down_up(x)\n",
    "            elif o == \"freq_blur\":\n",
    "                x = self._freq_blur(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 5. 손실 계산 시 mel 길이 mismatch 보정 유틸 =====\n",
    "def match_mel_for_loss(mel_a, mel_b):\n",
    "    \"\"\"\n",
    "    두 mel 텐서 (B,F,T)를 중앙 crop해서 같은 (F',T')로 만든다.\n",
    "    content loss에서 mel_adv vs mel_clean 비교용.\n",
    "    \"\"\"\n",
    "    A4 = mel_a.unsqueeze(1)  # (B,1,F,T)\n",
    "    B4 = mel_b.unsqueeze(1)  # (B,1,F,T)\n",
    "    A4c, B4c = match_spatial(A4, B4)\n",
    "    A3c = A4c.squeeze(1)     # (B,F',T')\n",
    "    B3c = B4c.squeeze(1)     # (B,F',T')\n",
    "    return A3c, B3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "# HiFi-GAN import path\n",
    "sys.path.append(\"./external/hifigan\")\n",
    "from models import Generator\n",
    "from env import AttrDict\n",
    "\n",
    "class HiFiGANVocoderWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    HiFi-GAN vocoder wrapper (UNIVERSAL_V1).\n",
    "    입력 : mel (B, 80, Tm)  [log-mel-ish]\n",
    "    출력 : wave_16k (B,1,T16k)\n",
    "\n",
    "    - gen 파라미터는 freeze (requires_grad=False)\n",
    "    - BUT forward는 no_grad 안 씀 -> gradient는 mel까지 거슬러 올라간다.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hifigan_config_path=\"./external/hifigan/UNIVERSAL_V1/config.json\",\n",
    "        hifigan_ckpt_path=\"./external/hifigan/UNIVERSAL_V1/g_02500000\",\n",
    "        sr_gen=22050,\n",
    "        sr_target=16000,\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.sr_gen = sr_gen\n",
    "        self.sr_target = sr_target\n",
    "\n",
    "        # 1) load config\n",
    "        with open(hifigan_config_path, \"r\") as f:\n",
    "            h = AttrDict(json.load(f))\n",
    "\n",
    "        # 2) init generator\n",
    "        gen = Generator(h).to(device)\n",
    "        gen.eval()\n",
    "\n",
    "        # 3) load weights\n",
    "        state = torch.load(hifigan_ckpt_path, map_location=device)\n",
    "        if \"generator\" in state:\n",
    "            gen.load_state_dict(state[\"generator\"])\n",
    "        else:\n",
    "            gen.load_state_dict(state)\n",
    "\n",
    "        # freeze params\n",
    "        for p in gen.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.gen = gen\n",
    "\n",
    "        # dummy stats (placeholder – 나중에 HiFi-GAN 학습 시 mel normalization 맞추면 여기 조정)\n",
    "        self.register_buffer(\"dummy_mean\", torch.tensor(0.0))\n",
    "        self.register_buffer(\"dummy_std\",  torch.tensor(1.0))\n",
    "\n",
    "    def preprocess_mel_for_hifigan(self, mel):\n",
    "        # mel: (B,80,T)\n",
    "        mel_norm = (mel - self.dummy_mean) / (self.dummy_std + 1e-8)\n",
    "        return mel_norm\n",
    "\n",
    "    def forward(self, mel_batch):\n",
    "        \"\"\"\n",
    "        mel_batch: (B, 80, Tm)\n",
    "        return   : (B, 1, T_16k)\n",
    "        \"\"\"\n",
    "        mel_in = self.preprocess_mel_for_hifigan(mel_batch).to(self.device)\n",
    "        # NO torch.no_grad(): keep graph\n",
    "        wave_gen_22k = self.gen(mel_in)  # (B,1,T22k)\n",
    "\n",
    "        # resample 22.05k ->16k\n",
    "        if self.sr_gen != self.sr_target:\n",
    "            wave_16k = torchaudio.functional.resample(\n",
    "                wave_gen_22k.squeeze(1),  # (B,T22k)\n",
    "                orig_freq=self.sr_gen,\n",
    "                new_freq=self.sr_target\n",
    "            ).unsqueeze(1)               # (B,1,T16k)\n",
    "        else:\n",
    "            wave_16k = wave_gen_22k\n",
    "\n",
    "        wave_16k = torch.clamp(wave_16k, -1.0, 1.0)\n",
    "        return wave_16k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    conv -> BN -> LeakyReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p)\n",
    "        self.bn   = nn.BatchNorm2d(out_ch)\n",
    "        self.act  = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class GatedSkipFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    skip 특징맵(skip_feat)과 upsample된 특징(up_feat)을 결합할 때,\n",
    "    skip_feat에 학습 가능한 게이트를 곱해 speaker-specific 정보를\n",
    "    부분적으로 억제하거나 왜곡하도록 만든다.\n",
    "\n",
    "    up_feat:   (B, C_up, H, W)\n",
    "    skip_feat: (B, C_skip, H, W)  (match_spatial로 맞춘 뒤 들어온다고 가정)\n",
    "\n",
    "    출력: concat([up_feat, gated_skip], dim=1)\n",
    "    \"\"\"\n",
    "    def __init__(self, c_skip, c_up):\n",
    "        super().__init__()\n",
    "        # 게이트를 만들 1x1 conv -> sigmoid\n",
    "        self.gate_gen = nn.Sequential(\n",
    "            nn.Conv2d(c_skip + c_up, c_skip, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, up_feat, skip_feat):\n",
    "        # 크기가 약간 다를 수 있으므로 맞춰준다.\n",
    "        up_m, skip_m = match_spatial(up_feat, skip_feat)  # (B,C_up,H',W'), (B,C_skip,H',W')\n",
    "\n",
    "        # 게이트 계산: concat해서 gate 만들고 skip에 곱한다.\n",
    "        gate_in = torch.cat([up_m, skip_m], dim=1)        # (B, C_up+C_skip, H', W')\n",
    "        gate    = self.gate_gen(gate_in)                  # (B, C_skip, H', W')\n",
    "        skip_g  = skip_m * gate                           # (B, C_skip, H', W')\n",
    "\n",
    "        fused   = torch.cat([up_m, skip_g], dim=1)        # (B, C_up+C_skip, H', W')\n",
    "        return fused\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 개선된 U-Net 생성기\n",
    "#########################################\n",
    "\n",
    "class UNetLikeGeneratorV2(nn.Module):\n",
    "    \"\"\"\n",
    "    개선된 생성기:\n",
    "    - Down/Up path를 3단으로 늘려 receptive field 확장\n",
    "    - Gated skip connection으로 speaker identity 누수 줄임\n",
    "    - 출력 perturbation에 ε * tanh(...)를 적용해 magnitude 제한\n",
    "    - match_spatial을 사용해 off-by-one 처리\n",
    "\n",
    "    입력:  mel_batch (B, n_mels, T)\n",
    "    출력:  adv_mel   (B, n_mels, T')\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=64, n_mels=80, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.n_mels  = n_mels\n",
    "        self.epsilon = epsilon  # perturbation 최대 크기를 제어하는 하이퍼파라미터\n",
    "\n",
    "        # ========== Encoder Blocks ==========\n",
    "        # enc1/down1\n",
    "        self.enc1 = nn.Sequential(\n",
    "            ConvBlock(1, base_ch),\n",
    "            ConvBlock(base_ch, base_ch)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(\n",
    "            base_ch, base_ch*2,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # /2\n",
    "\n",
    "        # enc2/down2\n",
    "        self.enc2 = nn.Sequential(\n",
    "            ConvBlock(base_ch*2, base_ch*2),\n",
    "            ConvBlock(base_ch*2, base_ch*2)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(\n",
    "            base_ch*2, base_ch*4,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # /4\n",
    "\n",
    "        # enc3/down3\n",
    "        self.enc3 = nn.Sequential(\n",
    "            ConvBlock(base_ch*4, base_ch*4),\n",
    "            ConvBlock(base_ch*4, base_ch*4)\n",
    "        )\n",
    "        self.down3 = nn.Conv2d(\n",
    "            base_ch*4, base_ch*8,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # /8\n",
    "\n",
    "        # bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ConvBlock(base_ch*8, base_ch*8),\n",
    "            ConvBlock(base_ch*8, base_ch*8)\n",
    "        )\n",
    "\n",
    "        # ========== Decoder Blocks ==========\n",
    "        # up3: from bottleneck -> scale x2\n",
    "        self.up3 = nn.ConvTranspose2d(\n",
    "            base_ch*8, base_ch*4,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # *4 (상대적으로 bottleneck에서 /8 -> /4)\n",
    "        self.skip_fuse3 = GatedSkipFusion(c_skip=base_ch*4, c_up=base_ch*4)\n",
    "        # 결과 채널 수는 base_ch*8\n",
    "        self.dec3 = nn.Sequential(\n",
    "            ConvBlock(base_ch*8, base_ch*4),\n",
    "            ConvBlock(base_ch*4, base_ch*4)\n",
    "        )\n",
    "\n",
    "        # up2: from dec3 -> scale x2\n",
    "        self.up2 = nn.ConvTranspose2d(\n",
    "            base_ch*4, base_ch*2,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # *8 -> /2 scale 대비\n",
    "        self.skip_fuse2 = GatedSkipFusion(c_skip=base_ch*2, c_up=base_ch*2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            ConvBlock(base_ch*4, base_ch*2),\n",
    "            ConvBlock(base_ch*2, base_ch*2)\n",
    "        )\n",
    "\n",
    "        # up1: from dec2 -> scale x2\n",
    "        self.up1 = nn.ConvTranspose2d(\n",
    "            base_ch*2, base_ch,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # back to ~original res\n",
    "        self.skip_fuse1 = GatedSkipFusion(c_skip=base_ch, c_up=base_ch)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            ConvBlock(base_ch*2, base_ch),\n",
    "            ConvBlock(base_ch, base_ch)\n",
    "        )\n",
    "\n",
    "        # 최종 1x1 conv로 perturbation(delta) 생성\n",
    "        self.out_conv = nn.Conv2d(base_ch, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, mel_batch):\n",
    "        \"\"\"\n",
    "        mel_batch: (B, n_mels, T)\n",
    "        return    : adv_mel (B, n_mels, T_adj)\n",
    "        \"\"\"\n",
    "        B, Freq, Time = mel_batch.shape  # Freq~80, Time~variable\n",
    "\n",
    "        x = mel_batch.unsqueeze(1)  # (B,1,Freq,Time)\n",
    "\n",
    "        # ----- Encoder -----\n",
    "        e1 = self.enc1(x)            # (B,base_ch,       F,   T)\n",
    "        d1 = self.down1(e1)          # (B,base_ch*2,     F/2, T/2)\n",
    "\n",
    "        e2 = self.enc2(d1)           # (B,base_ch*2,     F/2, T/2)\n",
    "        d2 = self.down2(e2)          # (B,base_ch*4,     F/4, T/4)\n",
    "\n",
    "        e3 = self.enc3(d2)           # (B,base_ch*4,     F/4, T/4)\n",
    "        d3 = self.down3(e3)          # (B,base_ch*8,     F/8, T/8)\n",
    "\n",
    "        bott = self.bottleneck(d3)   # (B,base_ch*8,     F/8, T/8)\n",
    "\n",
    "        # ----- Decoder -----\n",
    "        u3 = self.up3(bott)          # (B,base_ch*4,     F/4?,T/4?)\n",
    "        # gated skip with e3\n",
    "        fuse3 = self.skip_fuse3(u3, e3)  # (B,base_ch*8, F/4?,T/4?)\n",
    "        dec3  = self.dec3(fuse3)         # (B,base_ch*4, F/4?,T/4?)\n",
    "\n",
    "        u2 = self.up2(dec3)          # (B,base_ch*2,     F/2?,T/2?)\n",
    "        fuse2 = self.skip_fuse2(u2, e2)  # (B,base_ch*4, F/2?,T/2?)\n",
    "        dec2  = self.dec2(fuse2)         # (B,base_ch*2, F/2?,T/2?)\n",
    "\n",
    "        u1 = self.up1(dec2)          # (B,base_ch,       F?,  T?)\n",
    "        fuse1 = self.skip_fuse1(u1, e1)  # (B,base_ch*2, F?, T?)\n",
    "        dec1  = self.dec1(fuse1)         # (B,base_ch,   F?, T?)\n",
    "\n",
    "        delta = self.out_conv(dec1)      # (B,1,F_out,T_out)\n",
    "\n",
    "        # perturbation magnitude 제한: ε * tanh(delta)\n",
    "        delta_limited = self.epsilon * torch.tanh(delta)\n",
    "\n",
    "        # 원본 mel과 residual add\n",
    "        mel_as_img = mel_batch.unsqueeze(1)  # (B,1,Freq,Time)\n",
    "\n",
    "        delta_adj, mel_adj = match_spatial(delta_limited, mel_as_img)\n",
    "        adv_mel = mel_adj + delta_adj        # (B,1,F_aligned,T_aligned)\n",
    "        adv_mel = adv_mel.squeeze(1)         # (B,F_aligned,T_aligned)\n",
    "\n",
    "        return adv_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "c:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\site-packages\\speechbrain\\utils\\autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "Epoch 1/2:   1%|          | 4/676 [03:01<13:16:36, 71.13s/it, L_total=-0.297, L_cont=0.016, L_resist=-0.375, cos_sim=0.375]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################################\n",
    "# 1. 유틸 함수들 (이미 위에서 정의한 걸 그대로 사용)\n",
    "########################################################\n",
    "\n",
    "def match_wave_for_ecapa(wa, wb):\n",
    "    \"\"\"\n",
    "    wa, wb: (B,1,T)\n",
    "    길이가 다르면 가운데 기준으로 crop해서 동일 길이로 맞춘다.\n",
    "    \"\"\"\n",
    "    Ta = wa.shape[-1]\n",
    "    Tb = wb.shape[-1]\n",
    "    if Ta == Tb:\n",
    "        return wa, wb\n",
    "    if Ta > Tb:\n",
    "        start = (Ta - Tb) // 2\n",
    "        wa = wa[..., start:start+Tb]\n",
    "    else:\n",
    "        start = (Tb - Ta) // 2\n",
    "        wb = wb[..., start:start+Ta]\n",
    "    return wa, wb\n",
    "\n",
    "def match_spatial(src, ref):\n",
    "    \"\"\"\n",
    "    src, ref: (B,C,H,W)\n",
    "    두 텐서를 가운데 중심으로 크롭해서 같은 (H',W')로 만든다.\n",
    "    \"\"\"\n",
    "    B1,C1,Hs,Ws = src.shape\n",
    "    B2,C2,Hr,Wr = ref.shape\n",
    "    assert B1 == B2, \"batch mismatch in match_spatial\"\n",
    "\n",
    "    Ht = min(Hs, Hr)\n",
    "    Wt = min(Ws, Wr)\n",
    "\n",
    "    def center_crop(t, Ht, Wt):\n",
    "        _,_,H,W = t.shape\n",
    "        start_h = (H - Ht)//2\n",
    "        start_w = (W - Wt)//2\n",
    "        return t[:,:,start_h:start_h+Ht, start_w:start_w+Wt]\n",
    "\n",
    "    src_c = center_crop(src, Ht, Wt)\n",
    "    ref_c = center_crop(ref, Ht, Wt)\n",
    "    return src_c, ref_c\n",
    "\n",
    "def match_mel_for_loss(mel_a, mel_b):\n",
    "    \"\"\"\n",
    "    mel_a, mel_b: (B,F,T)\n",
    "    mel_adv vs mel_clean 비교용으로 중앙 crop해서 동일 크기로 맞춘다.\n",
    "    \"\"\"\n",
    "    A4 = mel_a.unsqueeze(1)  # (B,1,F,T)\n",
    "    B4 = mel_b.unsqueeze(1)  # (B,1,F,T)\n",
    "    A4c, B4c = match_spatial(A4, B4)\n",
    "    A3c = A4c.squeeze(1)     # (B,F',T')\n",
    "    B3c = B4c.squeeze(1)     # (B,F',T')\n",
    "    return A3c, B3c\n",
    "\n",
    "########################################################\n",
    "# 2. 하이퍼파라미터 및 모델 초기화\n",
    "########################################################\n",
    "\n",
    "lambda_c = 5.0   # content 보존 가중치\n",
    "lambda_r = 1.0   # speaker 유사도 붕괴 가중치\n",
    "lr = 1e-4\n",
    "num_epochs = 2   # 데모용\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 생성기 (개선된 U-Net)\n",
    "G = UNetLikeGeneratorV2(\n",
    "        base_ch=64,\n",
    "        n_mels=80,\n",
    "        epsilon=0.1\n",
    "    ).to(device)\n",
    "\n",
    "# purifier (방어 측 정화 시뮬레이션)\n",
    "D_purify = StochasticPurifier().to(device)\n",
    "\n",
    "# HiFi-GAN vocoder wrapper (freeze되지만 grad 경로는 유지)\n",
    "V_vocoder = HiFiGANVocoderWrapper(\n",
    "    hifigan_config_path = \"./external/hifigan/UNIVERSAL_V1/config.json\",\n",
    "    hifigan_ckpt_path   = \"./external/hifigan/UNIVERSAL_V1/g_02500000\",\n",
    "    sr_gen              = 22050,\n",
    "    sr_target           = 16000,\n",
    "    device              = device\n",
    ").to(device)\n",
    "\n",
    "# ECAPA-TDNN speaker encoder (freeze된 판별자, grad는 입력까지 흘러감)\n",
    "ECAPA = ECAPASpeakerEncoder(device=device).to(device)\n",
    "\n",
    "# 옵티마이저: G만 학습 (D_purify, vocoder, ECAPA는 freeze)\n",
    "optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "\n",
    "# 텐서보드 로거\n",
    "writer = SummaryWriter(log_dir=f\"./runs/exp_tqdm\")\n",
    "global_step = 0\n",
    "\n",
    "########################################################\n",
    "# 3. 학습 루프 with tqdm\n",
    "########################################################\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # tqdm으로 loader 감싸기. desc에 현재 epoch 표시.\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", dynamic_ncols=True)\n",
    "\n",
    "    for batch in pbar:\n",
    "        # -------------------------------------------------\n",
    "        # 1. clean waveform (16kHz 기준)\n",
    "        wave_clean = batch[\"waveform\"].to(device)  # (B,1,T)\n",
    "\n",
    "        # 2. mel 변환 (현재 버전: 16k 파형 -> log-mel(80bin))\n",
    "        #    torchaudio 변환기는 CPU에서 돌고 나서 .to(device)\n",
    "        mel_clean_cpu = wav_to_mel_db(wave_clean.cpu())   # (B,80,Tmel_clean)\n",
    "        mel_clean = mel_clean_cpu.to(device)\n",
    "\n",
    "        # 3. 생성기 G로 보호 멜 생성\n",
    "        mel_adv = G(mel_clean)                # (B,80,Tmel_adv)\n",
    "\n",
    "        # 4. purification-aware 정화자\n",
    "        mel_purified = D_purify(mel_adv)      # (B,80,Tmel_pur)\n",
    "\n",
    "        # 5. vocoder: mel_purified -> adversarial waveform (B,1,T_adv_16k)\n",
    "        wave_adv = V_vocoder(mel_purified)\n",
    "\n",
    "        # 6. 길이 맞추기 (ECAPA cosine 비교 전에 crop)\n",
    "        wave_adv_m, wave_clean_m = match_wave_for_ecapa(wave_adv, wave_clean)\n",
    "\n",
    "        # 7. ECAPA 임베딩 추출\n",
    "        emb_orig     = ECAPA(wave_clean_m)    # (B,emb_dim)\n",
    "        emb_purified = ECAPA(wave_adv_m)      # (B,emb_dim)\n",
    "\n",
    "        # 8. Content loss (mel 수준 L1)\n",
    "        mel_adv_crop, mel_clean_crop = match_mel_for_loss(mel_adv, mel_clean)\n",
    "        L_content = F.l1_loss(mel_adv_crop, mel_clean_crop)\n",
    "\n",
    "        # 9. Resist loss (화자 유사도 ↓)\n",
    "        cos_sim_vec = torch.sum(\n",
    "            F.normalize(emb_orig, p=2, dim=1) *\n",
    "            F.normalize(emb_purified, p=2, dim=1),\n",
    "            dim=1\n",
    "        )  # (B,)\n",
    "        L_resist = - torch.mean(cos_sim_vec)\n",
    "\n",
    "        # 10. total loss\n",
    "        L_total = lambda_c * L_content + lambda_r * L_resist\n",
    "\n",
    "        # 11. backward & step\n",
    "        optimizer.zero_grad()\n",
    "        L_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 12. 텐서보드 로깅\n",
    "        with torch.no_grad():\n",
    "            mean_cos = torch.mean(cos_sim_vec).item()\n",
    "            writer.add_scalar(\"loss/total\",     L_total.item(),    global_step)\n",
    "            writer.add_scalar(\"loss/content\",   L_content.item(),  global_step)\n",
    "            writer.add_scalar(\"loss/resist\",    L_resist.item(),   global_step)\n",
    "            writer.add_scalar(\"metric/cos_sim\", mean_cos,          global_step)\n",
    "            writer.add_scalar(\n",
    "                \"metric/delta_Tmel\",\n",
    "                float(mel_clean.shape[-1] - mel_adv.shape[-1]),\n",
    "                global_step\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"metric/wave_len_diff\",\n",
    "                float(wave_clean.shape[-1] - wave_adv.shape[-1]),\n",
    "                global_step\n",
    "            )\n",
    "\n",
    "        # 13. tqdm 진행바에 현재 loss 표시\n",
    "        pbar.set_postfix({\n",
    "            \"L_total\":   f\"{L_total.item():.3f}\",\n",
    "            \"L_cont\":    f\"{L_content.item():.3f}\",\n",
    "            \"L_resist\":  f\"{L_resist.item():.3f}\",\n",
    "            \"cos_sim\":   f\"{mean_cos:.3f}\",\n",
    "        })\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "writer.close()\n",
    "print(\"ECAPA+HiFiGAN 정화-aware 학습 루프 (tqdm 포함) 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
