{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "C:\\Users\\mmc\\AppData\\Local\\Temp\\ipykernel_54596\\4110274581.py:12: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import random\n",
    "\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIBRISPEECH_ROOT = c:\\project\\vcshield\\data\\train\\dev-clean\n"
     ]
    }
   ],
   "source": [
    "#config\n",
    "####################################\n",
    "# 경로 및 공통 설정\n",
    "####################################\n",
    "\n",
    "# 실험 노트북(.ipynb)이 있는 위치를 기준으로 상대경로를 잡는다고 가정\n",
    "PROJECT_ROOT = os.path.abspath(\".\")  # 필요하면 직접 바꿔도 됨\n",
    "\n",
    "LIBRISPEECH_ROOT = os.path.join(PROJECT_ROOT, \"data\", \"train\", \"dev-clean\")\n",
    "# 예: ./data/train/dev-clean/84/..., 174/..., SPEAKERS.TXT ...\n",
    "\n",
    "print(\"LIBRISPEECH_ROOT =\", LIBRISPEECH_ROOT)\n",
    "assert os.path.isdir(LIBRISPEECH_ROOT), \"dev-clean 경로를 확인하세요.\"\n",
    "\n",
    "# 오디오 파라미터\n",
    "SAMPLE_RATE = 16000  # LibriSpeech 기본 16kHz\n",
    "\n",
    "# DataLoader 파라미터\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0  # 윈도우면 0~2 정도로, 리눅스면 더 올려도 됨\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "\n",
    "# 특정 화자만 사용할 경우 지정 (None이면 전체 화자)\n",
    "# 예: speaker_list = [\"84\", \"174\"]\n",
    "speaker_list = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibriSpeechSpeakerDataset] speakers: 40, utterances(raw): 2703\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "class LibriSpeechSpeakerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    LibriSpeech dev-clean에서 화자 단위로 발화들을 로드한다.\n",
    "    각 item은 dict:\n",
    "        {\n",
    "          \"waveform\": Tensor shape (1, T) float32 [-1,1],\n",
    "          \"speaker_id\": str,\n",
    "          \"utt_path\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        target_sr: int = 16000,\n",
    "        speaker_list: Optional[List[str]] = None,\n",
    "        min_duration_sec: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        root_dir: dev-clean 경로\n",
    "        target_sr: 리샘플할 샘플레이트 (보통 16k)\n",
    "        speaker_list: 사용할 화자 ID 리스트. None이면 root_dir 내 모든 화자 ID 사용.\n",
    "        min_duration_sec: 너무 짧은 음성을 버리기 위한 최소 길이(초)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.target_sr = target_sr\n",
    "        self.min_duration_sec = min_duration_sec\n",
    "\n",
    "        # 1) 화자 폴더 수집 (폴더명이 전부 숫자인 것만 화자로 간주)\n",
    "        all_speakers = [\n",
    "            d for d in os.listdir(root_dir)\n",
    "            if os.path.isdir(os.path.join(root_dir, d)) and d.isdigit()\n",
    "        ]\n",
    "\n",
    "        if speaker_list is None:\n",
    "            self.speakers = sorted(all_speakers)\n",
    "        else:\n",
    "            # 교집합만 취함\n",
    "            self.speakers = sorted([s for s in all_speakers if s in speaker_list])\n",
    "\n",
    "        # 2) 각 화자에서 실제 오디오 파일(.flac/.wav) 경로 모으기\n",
    "        #    LibriSpeech는 일반적으로 spk_id/chapter_id/*.flac 형태\n",
    "        self.items = []\n",
    "        for spk in self.speakers:\n",
    "            spk_dir = os.path.join(root_dir, spk)\n",
    "            # 챕터 디렉토리들\n",
    "            for ch_name in os.listdir(spk_dir):\n",
    "                ch_dir = os.path.join(spk_dir, ch_name)\n",
    "                if not os.path.isdir(ch_dir):\n",
    "                    continue\n",
    "\n",
    "                # .wav / .flac 다 지원\n",
    "                wav_paths = glob.glob(os.path.join(ch_dir, \"*.wav\"))\n",
    "                flac_paths = glob.glob(os.path.join(ch_dir, \"*.flac\"))\n",
    "                audio_paths = sorted(wav_paths + flac_paths)\n",
    "\n",
    "                for ap in audio_paths:\n",
    "                    # 길이 필터링을 위해 일단 메타만 등록하고,\n",
    "                    # 실제 __getitem__에서 필요하면 필터하자.\n",
    "                    self.items.append({\n",
    "                        \"speaker_id\": spk,\n",
    "                        \"utt_path\": ap\n",
    "                    })\n",
    "\n",
    "        print(f\"[LibriSpeechSpeakerDataset] speakers: {len(self.speakers)}, utterances(raw): {len(self.items)}\")\n",
    "\n",
    "        # 사전 길이 필터를 적용해도 되지만, 여기서는 __getitem__에서 처리 실패 시 재시도 하기보단\n",
    "        # 그냥 __len__/__getitem__이 일관되게 동작하도록 그대로 둔다.\n",
    "        # (필요하면 나중에 pre-filter 로직 추가 가능)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def _load_audio(self, path: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        path에서 오디오 로딩하고 mono+resample까지 맞춰서 (1, T) float32 [-1,1] 반환\n",
    "        \"\"\"\n",
    "        wav, sr = torchaudio.load(path)  # wav: (C, T), float32 -1~1 범위일 가능성 높음\n",
    "        # 모노화\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "        # 리샘플\n",
    "        if sr != self.target_sr:\n",
    "            wav = torchaudio.functional.resample(wav, sr, self.target_sr)\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        meta = self.items[idx]\n",
    "        spk = meta[\"speaker_id\"]\n",
    "        path = meta[\"utt_path\"]\n",
    "\n",
    "        wav = self._load_audio(path)  # (1, T)\n",
    "\n",
    "        # 너무 짧으면(말 없는 부분 등) downstream에서 학습이 불안정할 수 있으므로 여기서 잘라낼 수 있다.\n",
    "        # 여기서는 min_duration_sec 이상만 보장하도록 잘라주는 정도만 (필요하다면)\n",
    "        min_len = int(self.min_duration_sec * self.target_sr)\n",
    "        if wav.shape[1] < min_len:\n",
    "            # 너무 짧다면 패딩 혹은 스킵 로직을 짤 수도 있다.\n",
    "            # 간단하게는 zero-pad\n",
    "            pad_len = min_len - wav.shape[1]\n",
    "            wav = torch.cat([wav, torch.zeros((1, pad_len), dtype=wav.dtype)], dim=1)\n",
    "\n",
    "        # MAX_SEC = 3.0\n",
    "        # max_len = int(MAX_SEC * self.target_sr)\n",
    "        # if wav.shape[1] > max_len:\n",
    "        #     start = (wav.shape[1] - max_len) // 2\n",
    "        #     wav = wav[:, start:start+max_len]\n",
    "\n",
    "        return {\n",
    "            \"waveform\": wav,       # (1, T_resampled)\n",
    "            \"speaker_id\": spk,     # string\n",
    "            \"utt_path\": path       # string\n",
    "        }\n",
    "    \n",
    "dataset = LibriSpeechSpeakerDataset(\n",
    "    root_dir=LIBRISPEECH_ROOT,\n",
    "    target_sr=SAMPLE_RATE,\n",
    "    speaker_list=speaker_list,   # None이면 전체 speaker\n",
    "    min_duration_sec=0.5         # 너무 짧은 샘플은 최소 0.5초 길이로 패딩\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from speechbrain.utils.fetching import LocalStrategy\n",
    "\n",
    "class ECAPASpeakerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ECAPA-TDNN speaker encoder wrapper.\n",
    "    - freeze params (no finetune)\n",
    "    - BUT allow gradients to flow w.r.t. the input waveform\n",
    "      so L_resist can push G through vocoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.classifier = EncoderClassifier.from_hparams(\n",
    "            source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "            run_opts={\"device\": device},\n",
    "            savedir=\"./pretrained_ecapa\",\n",
    "            local_strategy=LocalStrategy.COPY,  # <= Windows safe (copies instead of symlink) :contentReference[oaicite:1]{index=1}\n",
    "        )\n",
    "\n",
    "        # freeze ECAPA weights so we don't fine-tune it\n",
    "        for p in self.classifier.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, wave_batch):\n",
    "        \"\"\"\n",
    "        wave_batch: (B,1,T) float32 @16kHz\n",
    "        returns: (B, emb_dim)\n",
    "        NOTE: no torch.no_grad() here. We WANT grad to flow back\n",
    "              from cosine loss into wave_batch (→ vocoder → G).\n",
    "        \"\"\"\n",
    "        if wave_batch.dim() == 3 and wave_batch.shape[1] == 1:\n",
    "            wave_in = wave_batch.squeeze(1)  # (B,T)\n",
    "        else:\n",
    "            wave_in = wave_batch            # assume already (B,T)\n",
    "\n",
    "        # encode_batch returns (B,1,emb_dim)\n",
    "        emb = self.classifier.encode_batch(wave_in)\n",
    "        emb = emb.squeeze(1)               # (B, emb_dim)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sanity Check w/ collate_fn ===\n",
      "waveform.shape: torch.Size([4, 1, 208480])\n",
      "speaker_id example: ['2277', '5338', '251', '1673']\n",
      "utt_path[0]: c:\\project\\vcshield\\data\\train\\dev-clean\\2277\\149897\\2277-149897-0004.flac\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def match_spatial(src, ref):\n",
    "    \"\"\"\n",
    "    src, ref: 4D 텐서 (B,C,H,W)\n",
    "    리턴: src와 ref 중 작은 쪽 크기로 둘을 맞춰서 (src_adj, ref_adj)를 돌려준다.\n",
    "    - 만약 H/W가 서로 다르면\n",
    "      중앙을 기준으로 잘라서 동일한 (H',W')로 맞춤\n",
    "    - src, ref 중 어느 쪽이 더 큰지/작은지는 각각 축마다 독립적으로 처리\n",
    "    \"\"\"\n",
    "    B1,C1,Hs,Ws = src.shape\n",
    "    B2,C2,Hr,Wr = ref.shape\n",
    "    assert B1 == B2, \"batch mismatch in match_spatial\"\n",
    "\n",
    "    # 최종 목표 크기\n",
    "    Ht = min(Hs, Hr)\n",
    "    Wt = min(Ws, Wr)\n",
    "\n",
    "    def center_crop(t, Ht, Wt):\n",
    "        _,_,H,W = t.shape\n",
    "        start_h = (H - Ht)//2\n",
    "        start_w = (W - Wt)//2\n",
    "        return t[:,:,start_h:start_h+Ht, start_w:start_w+Wt]\n",
    "\n",
    "    src_c = center_crop(src, Ht, Wt)\n",
    "    ref_c = center_crop(ref, Ht, Wt)\n",
    "    return src_c, ref_c\n",
    "\n",
    "def collate_with_padding(batch_list):\n",
    "    \"\"\"\n",
    "    batch_list는 __getitem__에서 나온 dict들의 리스트 (len = B)\n",
    "\n",
    "    목표:\n",
    "    - waveform들을 가장 긴 샘플 길이에 맞춰 zero-pad (right padding)\n",
    "    - speaker_id / utt_path는 리스트 그대로 유지\n",
    "    \"\"\"\n",
    "    wave_list = [item[\"waveform\"] for item in batch_list]  # [(1, T_i), ...]\n",
    "    spk_list = [item[\"speaker_id\"] for item in batch_list]\n",
    "    path_list = [item[\"utt_path\"] for item in batch_list]\n",
    "\n",
    "    # 가장 긴 길이 구하기\n",
    "    max_len = max(w.shape[1] for w in wave_list)\n",
    "\n",
    "    # pad 후 스택\n",
    "    padded = []\n",
    "    for w in wave_list:\n",
    "        if w.shape[1] < max_len:\n",
    "            pad_amount = max_len - w.shape[1]\n",
    "            w = F.pad(w, (0, pad_amount), mode=\"constant\", value=0.0)\n",
    "        padded.append(w)  # (1, max_len)\n",
    "    wave_tensor = torch.stack(padded, dim=0)  # (B, 1, max_len)\n",
    "\n",
    "    return {\n",
    "        \"waveform\": wave_tensor,\n",
    "        \"speaker_id\": spk_list,\n",
    "        \"utt_path\": path_list\n",
    "    }\n",
    "\n",
    "# collate_fn을 적용한 DataLoader\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_with_padding\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(\"=== Sanity Check w/ collate_fn ===\")\n",
    "print(\"waveform.shape:\", batch[\"waveform\"].shape)  # (B, 1, max_len_in_batch)\n",
    "print(\"speaker_id example:\", batch[\"speaker_id\"])\n",
    "print(\"utt_path[0]:\", batch[\"utt_path\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveform: torch.Size([4, 1, 113920])\n",
      "mel_db: torch.Size([4, 80, 446])\n",
      "speaker_id: ['6345', '422', '2277', '652']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def build_mel_extractor(\n",
    "    sample_rate=16000,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_mels=80,\n",
    "    f_min=0.0,\n",
    "    f_max=8000.0,\n",
    "    power=1.0,\n",
    "    log_offset=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    반환 함수 wav_to_mel_db:\n",
    "      입력  : waveform (B,1,T) 또는 (1,T)\n",
    "      출력  : mel_batch (B, n_mels, time)\n",
    "    \"\"\"\n",
    "\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        n_mels=n_mels,\n",
    "        power=power,\n",
    "    )\n",
    "\n",
    "    def wav_to_mel_db(waveform: torch.Tensor):\n",
    "        \"\"\"\n",
    "        waveform: (B,1,T) or (1,T)\n",
    "        returns: (B, n_mels, time)\n",
    "        \"\"\"\n",
    "        single_input = False\n",
    "\n",
    "        # case: (1,T) -> (1,1,T)\n",
    "        if waveform.dim() == 2:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "            single_input = True\n",
    "\n",
    "        # sanity check\n",
    "        if waveform.dim() != 3 or waveform.shape[1] != 1:\n",
    "            raise ValueError(f\"Expected (B,1,T) or (1,T); got {tuple(waveform.shape)}\")\n",
    "\n",
    "        B = waveform.size(0)\n",
    "        mel_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            # waveform[b]: (1, T)\n",
    "            mel = mel_transform(waveform[b])  # (1, n_mels, time)\n",
    "            # squeeze channel dim -> (n_mels, time)\n",
    "            mel = mel.squeeze(0)\n",
    "\n",
    "            mel_db = torch.log(mel + log_offset)  # (n_mels, time)\n",
    "            mel_list.append(mel_db)\n",
    "\n",
    "        # stack -> (B, n_mels, time)\n",
    "        mel_batch = torch.stack(mel_list, dim=0)\n",
    "\n",
    "        if single_input:\n",
    "            mel_batch = mel_batch[0]  # (n_mels, time)\n",
    "\n",
    "        return mel_batch\n",
    "\n",
    "    return wav_to_mel_db\n",
    "\n",
    "\n",
    "# 빌드\n",
    "wav_to_mel_db = build_mel_extractor(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_mels=80,\n",
    "    f_min=0.0,\n",
    "    f_max=8000.0,\n",
    "    power=1.0,\n",
    "    log_offset=1e-6\n",
    ")\n",
    "\n",
    "# 테스트\n",
    "test_batch = next(iter(loader))\n",
    "test_wave = test_batch[\"waveform\"]      # (B,1,T)\n",
    "test_mel  = wav_to_mel_db(test_wave)    # 기대: (B,80,time)\n",
    "\n",
    "print(\"waveform:\", test_wave.shape)\n",
    "print(\"mel_db:\", test_mel.shape)\n",
    "print(\"speaker_id:\", test_batch[\"speaker_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class StochasticPurifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 time_blur_kernel_sizes=(3,5,7),\n",
    "                 freq_blur_kernel_sizes=(3,5),\n",
    "                 downsample_factors=(2,3)):\n",
    "        super().__init__()\n",
    "        self.time_blur_kernel_sizes = time_blur_kernel_sizes\n",
    "        self.freq_blur_kernel_sizes = freq_blur_kernel_sizes\n",
    "        self.downsample_factors = downsample_factors\n",
    "\n",
    "    def _time_blur(self, mel):\n",
    "        B, Fm, Tm = mel.shape\n",
    "        k = random.choice(self.time_blur_kernel_sizes)\n",
    "        pad = k // 2\n",
    "        kernel = torch.ones(Fm, 1, k, device=mel.device, dtype=mel.dtype) / k\n",
    "        mel_pad = F.pad(mel, (pad, pad), mode='reflect')\n",
    "        out = F.conv1d(mel_pad, kernel, stride=1, padding=0, groups=Fm)\n",
    "        return out\n",
    "\n",
    "    def _time_down_up(self, mel):\n",
    "        B, Fm, Tm = mel.shape\n",
    "        factor = random.choice(self.downsample_factors)\n",
    "        if Tm // factor < 2:\n",
    "            return mel\n",
    "        T_down = max(2, Tm // factor)\n",
    "        mel_down = F.interpolate(\n",
    "            mel.unsqueeze(1),  # (B,1,Fm,Tm)\n",
    "            size=(Fm, T_down),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze(1)  # (B,Fm,T_down)\n",
    "        mel_up = F.interpolate(\n",
    "            mel_down.unsqueeze(1),\n",
    "            size=(Fm, Tm),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze(1)  # (B,Fm,Tm)\n",
    "        return mel_up\n",
    "\n",
    "    def _freq_blur(self, mel):\n",
    "        B, Fm, Tm = mel.shape\n",
    "        k = random.choice(self.freq_blur_kernel_sizes)\n",
    "        pad = k // 2\n",
    "\n",
    "        mel_t = mel.transpose(1,2)      # (B,Tm,Fm)\n",
    "        mel_bt = mel_t.reshape(B*Tm,1,Fm)\n",
    "        kernel = torch.ones(1,1,k, device=mel.device, dtype=mel.dtype)/k\n",
    "        mel_bt_pad = F.pad(mel_bt, (pad,pad), mode='reflect')\n",
    "        mel_bt_blur = F.conv1d(mel_bt_pad, kernel, stride=1, padding=0)\n",
    "        mel_blur_t = mel_bt_blur.reshape(B,Tm,Fm)\n",
    "        mel_blur = mel_blur_t.transpose(1,2)  # (B,Fm,Tm)\n",
    "        return mel_blur\n",
    "\n",
    "    def forward(self, mel):\n",
    "        ops = []\n",
    "        if random.random() < 0.7:\n",
    "            ops.append(\"time_blur\")\n",
    "        if random.random() < 0.5:\n",
    "            ops.append(\"time_down_up\")\n",
    "        if random.random() < 0.5:\n",
    "            ops.append(\"freq_blur\")\n",
    "\n",
    "        x = mel\n",
    "        for o in ops:\n",
    "            if o == \"time_blur\":\n",
    "                x = self._time_blur(x)\n",
    "            elif o == \"time_down_up\":\n",
    "                x = self._time_down_up(x)\n",
    "            elif o == \"freq_blur\":\n",
    "                x = self._freq_blur(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 5. 손실 계산 시 mel 길이 mismatch 보정 유틸 =====\n",
    "def match_mel_for_loss(mel_a, mel_b):\n",
    "    \"\"\"\n",
    "    두 mel 텐서 (B,F,T)를 중앙 crop해서 같은 (F',T')로 만든다.\n",
    "    content loss에서 mel_adv vs mel_clean 비교용.\n",
    "    \"\"\"\n",
    "    A4 = mel_a.unsqueeze(1)  # (B,1,F,T)\n",
    "    B4 = mel_b.unsqueeze(1)  # (B,1,F,T)\n",
    "    A4c, B4c = match_spatial(A4, B4)\n",
    "    A3c = A4c.squeeze(1)     # (B,F',T')\n",
    "    B3c = B4c.squeeze(1)     # (B,F',T')\n",
    "    return A3c, B3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "# HiFi-GAN import path\n",
    "sys.path.append(\"./external/hifigan\")\n",
    "from models import Generator\n",
    "from env import AttrDict\n",
    "\n",
    "class HiFiGANVocoderWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    HiFi-GAN vocoder wrapper (UNIVERSAL_V1).\n",
    "    입력 : mel (B, 80, Tm)  [log-mel-ish]\n",
    "    출력 : wave_16k (B,1,T16k)\n",
    "\n",
    "    - gen 파라미터는 freeze (requires_grad=False)\n",
    "    - BUT forward는 no_grad 안 씀 -> gradient는 mel까지 거슬러 올라간다.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hifigan_config_path=\"./external/hifigan/UNIVERSAL_V1/config.json\",\n",
    "        hifigan_ckpt_path=\"./external/hifigan/UNIVERSAL_V1/g_02500000\",\n",
    "        sr_gen=22050,\n",
    "        sr_target=16000,\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.sr_gen = sr_gen\n",
    "        self.sr_target = sr_target\n",
    "\n",
    "        # 1) load config\n",
    "        with open(hifigan_config_path, \"r\") as f:\n",
    "            h = AttrDict(json.load(f))\n",
    "\n",
    "        # 2) init generator\n",
    "        gen = Generator(h).to(device)\n",
    "        gen.eval()\n",
    "\n",
    "        # 3) load weights\n",
    "        state = torch.load(hifigan_ckpt_path, map_location=device)\n",
    "        if \"generator\" in state:\n",
    "            gen.load_state_dict(state[\"generator\"])\n",
    "        else:\n",
    "            gen.load_state_dict(state)\n",
    "\n",
    "        # freeze params\n",
    "        for p in gen.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.gen = gen\n",
    "\n",
    "        # dummy stats (placeholder – 나중에 HiFi-GAN 학습 시 mel normalization 맞추면 여기 조정)\n",
    "        self.register_buffer(\"dummy_mean\", torch.tensor(0.0))\n",
    "        self.register_buffer(\"dummy_std\",  torch.tensor(1.0))\n",
    "\n",
    "    def preprocess_mel_for_hifigan(self, mel):\n",
    "        # mel: (B,80,T)\n",
    "        mel_norm = (mel - self.dummy_mean) / (self.dummy_std + 1e-8)\n",
    "        return mel_norm\n",
    "\n",
    "    def forward(self, mel_batch):\n",
    "        \"\"\"\n",
    "        mel_batch: (B, 80, Tm)\n",
    "        return   : (B, 1, T_16k)\n",
    "        \"\"\"\n",
    "        mel_in = self.preprocess_mel_for_hifigan(mel_batch).to(self.device)\n",
    "        # NO torch.no_grad(): keep graph\n",
    "        wave_gen_22k = self.gen(mel_in)  # (B,1,T22k)\n",
    "\n",
    "        # resample 22.05k ->16k\n",
    "        if self.sr_gen != self.sr_target:\n",
    "            wave_16k = torchaudio.functional.resample(\n",
    "                wave_gen_22k.squeeze(1),  # (B,T22k)\n",
    "                orig_freq=self.sr_gen,\n",
    "                new_freq=self.sr_target\n",
    "            ).unsqueeze(1)               # (B,1,T16k)\n",
    "        else:\n",
    "            wave_16k = wave_gen_22k\n",
    "\n",
    "        wave_16k = torch.clamp(wave_16k, -1.0, 1.0)\n",
    "        return wave_16k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    conv -> BN -> LeakyReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p)\n",
    "        self.bn   = nn.BatchNorm2d(out_ch)\n",
    "        self.act  = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class GatedSkipFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    skip 특징맵(skip_feat)과 upsample된 특징(up_feat)을 결합할 때,\n",
    "    skip_feat에 학습 가능한 게이트를 곱해 speaker-specific 정보를\n",
    "    부분적으로 억제하거나 왜곡하도록 만든다.\n",
    "\n",
    "    up_feat:   (B, C_up, H, W)\n",
    "    skip_feat: (B, C_skip, H, W)  (match_spatial로 맞춘 뒤 들어온다고 가정)\n",
    "\n",
    "    출력: concat([up_feat, gated_skip], dim=1)\n",
    "    \"\"\"\n",
    "    def __init__(self, c_skip, c_up):\n",
    "        super().__init__()\n",
    "        # 게이트를 만들 1x1 conv -> sigmoid\n",
    "        self.gate_gen = nn.Sequential(\n",
    "            nn.Conv2d(c_skip + c_up, c_skip, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, up_feat, skip_feat):\n",
    "        # 크기가 약간 다를 수 있으므로 맞춰준다.\n",
    "        up_m, skip_m = match_spatial(up_feat, skip_feat)  # (B,C_up,H',W'), (B,C_skip,H',W')\n",
    "\n",
    "        # 게이트 계산: concat해서 gate 만들고 skip에 곱한다.\n",
    "        gate_in = torch.cat([up_m, skip_m], dim=1)        # (B, C_up+C_skip, H', W')\n",
    "        gate    = self.gate_gen(gate_in)                  # (B, C_skip, H', W')\n",
    "        skip_g  = skip_m * gate                           # (B, C_skip, H', W')\n",
    "\n",
    "        fused   = torch.cat([up_m, skip_g], dim=1)        # (B, C_up+C_skip, H', W')\n",
    "        return fused\n",
    "\n",
    "\n",
    "#########################################\n",
    "# 개선된 U-Net 생성기\n",
    "#########################################\n",
    "\n",
    "class UNetLikeGeneratorV2(nn.Module):\n",
    "    \"\"\"\n",
    "    개선된 생성기:\n",
    "    - Down/Up path를 3단으로 늘려 receptive field 확장\n",
    "    - Gated skip connection으로 speaker identity 누수 줄임\n",
    "    - 출력 perturbation에 ε * tanh(...)를 적용해 magnitude 제한\n",
    "    - match_spatial을 사용해 off-by-one 처리\n",
    "\n",
    "    입력:  mel_batch (B, n_mels, T)\n",
    "    출력:  adv_mel   (B, n_mels, T')\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=64, n_mels=80, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.n_mels  = n_mels\n",
    "        self.epsilon = epsilon  # perturbation 최대 크기를 제어하는 하이퍼파라미터\n",
    "\n",
    "        # ========== Encoder Blocks ==========\n",
    "        # enc1/down1\n",
    "        self.enc1 = nn.Sequential(\n",
    "            ConvBlock(1, base_ch),\n",
    "            ConvBlock(base_ch, base_ch)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(\n",
    "            base_ch, base_ch*2,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # /2\n",
    "\n",
    "        # enc2/down2\n",
    "        self.enc2 = nn.Sequential(\n",
    "            ConvBlock(base_ch*2, base_ch*2),\n",
    "            ConvBlock(base_ch*2, base_ch*2)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(\n",
    "            base_ch*2, base_ch*4,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # /4\n",
    "\n",
    "        # enc3/down3\n",
    "        self.enc3 = nn.Sequential(\n",
    "            ConvBlock(base_ch*4, base_ch*4),\n",
    "            ConvBlock(base_ch*4, base_ch*4)\n",
    "        )\n",
    "        self.down3 = nn.Conv2d(\n",
    "            base_ch*4, base_ch*8,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # /8\n",
    "\n",
    "        # bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ConvBlock(base_ch*8, base_ch*8),\n",
    "            ConvBlock(base_ch*8, base_ch*8)\n",
    "        )\n",
    "\n",
    "        # ========== Decoder Blocks ==========\n",
    "        # up3: from bottleneck -> scale x2\n",
    "        self.up3 = nn.ConvTranspose2d(\n",
    "            base_ch*8, base_ch*4,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # *4 (상대적으로 bottleneck에서 /8 -> /4)\n",
    "        self.skip_fuse3 = GatedSkipFusion(c_skip=base_ch*4, c_up=base_ch*4)\n",
    "        # 결과 채널 수는 base_ch*8\n",
    "        self.dec3 = nn.Sequential(\n",
    "            ConvBlock(base_ch*8, base_ch*4),\n",
    "            ConvBlock(base_ch*4, base_ch*4)\n",
    "        )\n",
    "\n",
    "        # up2: from dec3 -> scale x2\n",
    "        self.up2 = nn.ConvTranspose2d(\n",
    "            base_ch*4, base_ch*2,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # *8 -> /2 scale 대비\n",
    "        self.skip_fuse2 = GatedSkipFusion(c_skip=base_ch*2, c_up=base_ch*2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            ConvBlock(base_ch*4, base_ch*2),\n",
    "            ConvBlock(base_ch*2, base_ch*2)\n",
    "        )\n",
    "\n",
    "        # up1: from dec2 -> scale x2\n",
    "        self.up1 = nn.ConvTranspose2d(\n",
    "            base_ch*2, base_ch,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # back to ~original res\n",
    "        self.skip_fuse1 = GatedSkipFusion(c_skip=base_ch, c_up=base_ch)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            ConvBlock(base_ch*2, base_ch),\n",
    "            ConvBlock(base_ch, base_ch)\n",
    "        )\n",
    "\n",
    "        # 최종 1x1 conv로 perturbation(delta) 생성\n",
    "        self.out_conv = nn.Conv2d(base_ch, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, mel_batch):\n",
    "        \"\"\"\n",
    "        mel_batch: (B, n_mels, T)\n",
    "        return    : adv_mel (B, n_mels, T_adj)\n",
    "        \"\"\"\n",
    "        B, Freq, Time = mel_batch.shape  # Freq~80, Time~variable\n",
    "\n",
    "        x = mel_batch.unsqueeze(1)  # (B,1,Freq,Time)\n",
    "\n",
    "        # ----- Encoder -----\n",
    "        e1 = self.enc1(x)            # (B,base_ch,       F,   T)\n",
    "        d1 = self.down1(e1)          # (B,base_ch*2,     F/2, T/2)\n",
    "\n",
    "        e2 = self.enc2(d1)           # (B,base_ch*2,     F/2, T/2)\n",
    "        d2 = self.down2(e2)          # (B,base_ch*4,     F/4, T/4)\n",
    "\n",
    "        e3 = self.enc3(d2)           # (B,base_ch*4,     F/4, T/4)\n",
    "        d3 = self.down3(e3)          # (B,base_ch*8,     F/8, T/8)\n",
    "\n",
    "        bott = self.bottleneck(d3)   # (B,base_ch*8,     F/8, T/8)\n",
    "\n",
    "        # ----- Decoder -----\n",
    "        u3 = self.up3(bott)          # (B,base_ch*4,     F/4?,T/4?)\n",
    "        # gated skip with e3\n",
    "        fuse3 = self.skip_fuse3(u3, e3)  # (B,base_ch*8, F/4?,T/4?)\n",
    "        dec3  = self.dec3(fuse3)         # (B,base_ch*4, F/4?,T/4?)\n",
    "\n",
    "        u2 = self.up2(dec3)          # (B,base_ch*2,     F/2?,T/2?)\n",
    "        fuse2 = self.skip_fuse2(u2, e2)  # (B,base_ch*4, F/2?,T/2?)\n",
    "        dec2  = self.dec2(fuse2)         # (B,base_ch*2, F/2?,T/2?)\n",
    "\n",
    "        u1 = self.up1(dec2)          # (B,base_ch,       F?,  T?)\n",
    "        fuse1 = self.skip_fuse1(u1, e1)  # (B,base_ch*2, F?, T?)\n",
    "        dec1  = self.dec1(fuse1)         # (B,base_ch,   F?, T?)\n",
    "\n",
    "        delta = self.out_conv(dec1)      # (B,1,F_out,T_out)\n",
    "\n",
    "        # perturbation magnitude 제한: ε * tanh(delta)\n",
    "        delta_limited = self.epsilon * torch.tanh(delta)\n",
    "\n",
    "        # 원본 mel과 residual add\n",
    "        mel_as_img = mel_batch.unsqueeze(1)  # (B,1,Freq,Time)\n",
    "\n",
    "        delta_adj, mel_adj = match_spatial(delta_limited, mel_as_img)\n",
    "        adv_mel = mel_adj + delta_adj        # (B,1,F_aligned,T_aligned)\n",
    "        adv_mel = adv_mel.squeeze(1)         # (B,F_aligned,T_aligned)\n",
    "\n",
    "        return adv_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "c:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\site-packages\\speechbrain\\utils\\autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "[SpeedUp] Epoch 1/40:  22%|██▏       | 150/676 [00:43<02:34,  3.41it/s, L_tot=0.213, L_cont=0.001, L_res=0.207, cos_sub=0.207, idx=2] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 184\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# 8. backward & step\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    183\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 184\u001b[0m \u001b[43mL_total\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# 9. 로깅 & tqdm 표시\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmc\\miniconda3\\envs\\unlearn\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################################\n",
    "# 1. 유틸 함수들 (이미 위에서 정의한 걸 그대로 사용)\n",
    "########################################################\n",
    "\n",
    "def match_wave_for_ecapa(wa, wb):\n",
    "    \"\"\"\n",
    "    wa, wb: (B,1,T)\n",
    "    길이가 다르면 가운데 기준으로 crop해서 동일 길이로 맞춘다.\n",
    "    \"\"\"\n",
    "    Ta = wa.shape[-1]\n",
    "    Tb = wb.shape[-1]\n",
    "    if Ta == Tb:\n",
    "        return wa, wb\n",
    "    if Ta > Tb:\n",
    "        start = (Ta - Tb) // 2\n",
    "        wa = wa[..., start:start+Tb]\n",
    "    else:\n",
    "        start = (Tb - Ta) // 2\n",
    "        wb = wb[..., start:start+Ta]\n",
    "    return wa, wb\n",
    "\n",
    "def match_spatial(src, ref):\n",
    "    \"\"\"\n",
    "    src, ref: (B,C,H,W)\n",
    "    두 텐서를 가운데 중심으로 크롭해서 같은 (H',W')로 만든다.\n",
    "    \"\"\"\n",
    "    B1,C1,Hs,Ws = src.shape\n",
    "    B2,C2,Hr,Wr = ref.shape\n",
    "    assert B1 == B2, \"batch mismatch in match_spatial\"\n",
    "\n",
    "    Ht = min(Hs, Hr)\n",
    "    Wt = min(Ws, Wr)\n",
    "\n",
    "    def center_crop(t, Ht, Wt):\n",
    "        _,_,H,W = t.shape\n",
    "        start_h = (H - Ht)//2\n",
    "        start_w = (W - Wt)//2\n",
    "        return t[:,:,start_h:start_h+Ht, start_w:start_w+Wt]\n",
    "\n",
    "    src_c = center_crop(src, Ht, Wt)\n",
    "    ref_c = center_crop(ref, Ht, Wt)\n",
    "    return src_c, ref_c\n",
    "\n",
    "def match_mel_for_loss(mel_a, mel_b):\n",
    "    \"\"\"\n",
    "    mel_a, mel_b: (B,F,T)\n",
    "    mel_adv vs mel_clean 비교용으로 중앙 crop해서 동일 크기로 맞춘다.\n",
    "    \"\"\"\n",
    "    A4 = mel_a.unsqueeze(1)  # (B,1,F,T)\n",
    "    B4 = mel_b.unsqueeze(1)  # (B,1,F,T)\n",
    "    A4c, B4c = match_spatial(A4, B4)\n",
    "    A3c = A4c.squeeze(1)     # (B,F',T')\n",
    "    B3c = B4c.squeeze(1)     # (B,F',T')\n",
    "    return A3c, B3c\n",
    "\n",
    "########################################################\n",
    "# 2. 하이퍼파라미터 및 모델 초기화\n",
    "########################################################\n",
    "\n",
    "lambda_c = 5.0   # content 보존 가중치\n",
    "lambda_r = 1.0   # speaker 유사도 붕괴 가중치\n",
    "lr = 1e-4\n",
    "num_epochs = 40   # 예시\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "G = UNetLikeGeneratorV2(\n",
    "        base_ch=64,\n",
    "        n_mels=80,\n",
    "        epsilon=0.1\n",
    "    ).to(device)\n",
    "\n",
    "D_purify = StochasticPurifier().to(device)\n",
    "\n",
    "V_vocoder = HiFiGANVocoderWrapper(\n",
    "    hifigan_config_path = \"./external/hifigan/UNIVERSAL_V1/config.json\",\n",
    "    hifigan_ckpt_path   = \"./external/hifigan/UNIVERSAL_V1/g_02500000\",\n",
    "    sr_gen              = 22050,\n",
    "    sr_target           = 16000,\n",
    "    device              = device\n",
    ").to(device)\n",
    "\n",
    "ECAPA = ECAPASpeakerEncoder(device=device).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"./runs/exp_speedup\")\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "########################################################\n",
    "# 2. 유틸: 중앙 crop으로 wave 길이 맞추기\n",
    "########################################################\n",
    "def match_wave_for_ecapa(wa, wb):\n",
    "    \"\"\"\n",
    "    wa, wb: (B,1,T)\n",
    "    길이 다르면 중앙 crop해서 동일하게\n",
    "    \"\"\"\n",
    "    Ta = wa.shape[-1]\n",
    "    Tb = wb.shape[-1]\n",
    "    if Ta == Tb:\n",
    "        return wa, wb\n",
    "    if Ta > Tb:\n",
    "        start = (Ta - Tb) // 2\n",
    "        wa = wa[..., start:start+Tb]\n",
    "    else:\n",
    "        start = (Tb - Ta) // 2\n",
    "        wb = wb[..., start:start+Ta]\n",
    "    return wa, wb\n",
    "\n",
    "\n",
    "########################################################\n",
    "# 3. 학습 루프\n",
    "########################################################\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    pbar = tqdm(loader,\n",
    "                desc=f\"[SpeedUp] Epoch {epoch+1}/{num_epochs}\",\n",
    "                dynamic_ncols=True)\n",
    "\n",
    "    for batch in pbar:\n",
    "        # -------------------------------------------------\n",
    "        # 1. clean waveform (B,1,T_16k)  (이미 Dataset에서 3초 crop했다고 가정)\n",
    "        wave_clean = batch[\"waveform\"].to(device)\n",
    "\n",
    "        # 2. mel 변환\n",
    "        #    torchaudio 기반 mel 추출기는 CPU에서 돌리고 다시 device로 올리는 구조\n",
    "        mel_clean_cpu = wav_to_mel_db(wave_clean.cpu())   # (B,80,Tmel_clean)\n",
    "        mel_clean = mel_clean_cpu.to(device)\n",
    "\n",
    "        # 3. 생성기 G: 보호용 adversarial mel 생성\n",
    "        mel_adv = G(mel_clean)                # (B,80,Tmel_adv)\n",
    "\n",
    "        # 4. purification 방어 시뮬 (time-blur / down-up / freq-blur 등)\n",
    "        mel_purified = D_purify(mel_adv)      # (B,80,Tmel_pur)\n",
    "\n",
    "        # ----------------------------\n",
    "        # 5. Resist loss는 배치 전체가 아니라 \"무작위 1개\"만 사용\n",
    "        # ----------------------------\n",
    "        B = mel_purified.shape[0]\n",
    "        pick_idx = torch.randint(low=0, high=B, size=(1,)).item()\n",
    "\n",
    "        mel_purified_sub = mel_purified[pick_idx:pick_idx+1, :, :]  # (1,80,T')\n",
    "        wave_clean_sub   = wave_clean[  pick_idx:pick_idx+1, :, :]  # (1,1,Tclean)\n",
    "\n",
    "        # vocoder로 mel -> wave_adv (여긴 grad 유지: 이 1개 샘플만 비싸게 계산)\n",
    "        wave_adv_sub = V_vocoder(mel_purified_sub)  # (1,1,Tadv_16k)\n",
    "\n",
    "        # 길이 맞추기\n",
    "        wave_adv_m, wave_clean_m = match_wave_for_ecapa(wave_adv_sub, wave_clean_sub)\n",
    "\n",
    "        # ECAPA 임베딩\n",
    "        emb_orig_sub     = ECAPA(wave_clean_m)   # (1,emb_dim)\n",
    "        emb_purified_sub = ECAPA(wave_adv_m)     # (1,emb_dim)\n",
    "\n",
    "        # 화자 유사도 (cosine) -> 낮추고 싶다 => 음수로\n",
    "        cos_sim_sub = torch.sum(\n",
    "            F.normalize(emb_orig_sub, p=2, dim=1) *\n",
    "            F.normalize(emb_purified_sub, p=2, dim=1),\n",
    "            dim=1\n",
    "        )  # (1,)\n",
    "        L_resist = torch.mean(cos_sim_sub)  # 유사하면 클수록 큼\n",
    "\n",
    "        # ----------------------------\n",
    "        # 6. Content loss는 여전히 full batch로 계산\n",
    "        # ----------------------------\n",
    "        mel_adv_crop, mel_clean_crop = match_mel_for_loss(mel_adv, mel_clean)\n",
    "        L_content = F.l1_loss(mel_adv_crop, mel_clean_crop)\n",
    "\n",
    "        # ----------------------------\n",
    "        # 7. Total loss\n",
    "        # ----------------------------\n",
    "        L_total = lambda_c * L_content + lambda_r * L_resist\n",
    "\n",
    "        # ----------------------------\n",
    "        # 8. backward & step\n",
    "        # ----------------------------\n",
    "        optimizer.zero_grad()\n",
    "        L_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ----------------------------\n",
    "        # 9. 로깅 & tqdm 표시\n",
    "        # ----------------------------\n",
    "        with torch.no_grad():\n",
    "            # 참고용 metric: 우리가 실제로 cos_sim_sub (단일 샘플)만 썼음\n",
    "            mean_cos = cos_sim_sub.mean().item()\n",
    "\n",
    "            writer.add_scalar(\"loss/total\",     L_total.item(),    global_step)\n",
    "            writer.add_scalar(\"loss/content\",   L_content.item(),  global_step)\n",
    "            writer.add_scalar(\"loss/resist\",    L_resist.item(),   global_step)\n",
    "\n",
    "            writer.add_scalar(\"metric/cos_sim_sub\", mean_cos,      global_step)\n",
    "            writer.add_scalar(\"metric/used_idx\",     float(pick_idx), global_step)\n",
    "\n",
    "            # mel 길이 차이 로깅 (참고용)\n",
    "            writer.add_scalar(\"metric/delta_Tmel\",\n",
    "                              float(mel_clean.shape[-1] - mel_adv.shape[-1]),\n",
    "                              global_step)\n",
    "\n",
    "            # waveform 길이 차이도 한 샘플 기준\n",
    "            writer.add_scalar(\"metric/wave_len_diff_sub\",\n",
    "                              float(wave_clean_m.shape[-1] - wave_adv_m.shape[-1]),\n",
    "                              global_step)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"L_tot\":   f\"{L_total.item():.3f}\",\n",
    "            \"L_cont\":  f\"{L_content.item():.3f}\",\n",
    "            \"L_res\":   f\"{L_resist.item():.3f}\",\n",
    "            \"cos_sub\": f\"{mean_cos:.3f}\",\n",
    "            \"idx\":     int(pick_idx),\n",
    "        })\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "writer.close()\n",
    "print(\"속도 개선 루프 종료 (partial-batch resist + trimmed audio)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
