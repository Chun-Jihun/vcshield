{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIBRISPEECH_ROOT = c:\\project\\vcshield\\data\\train\\dev-clean\n"
     ]
    }
   ],
   "source": [
    "#config\n",
    "####################################\n",
    "# 경로 및 공통 설정\n",
    "####################################\n",
    "\n",
    "# 실험 노트북(.ipynb)이 있는 위치를 기준으로 상대경로를 잡는다고 가정\n",
    "PROJECT_ROOT = os.path.abspath(\".\")  # 필요하면 직접 바꿔도 됨\n",
    "\n",
    "LIBRISPEECH_ROOT = os.path.join(PROJECT_ROOT, \"data\", \"train\", \"dev-clean\")\n",
    "# 예: ./data/train/dev-clean/84/..., 174/..., SPEAKERS.TXT ...\n",
    "\n",
    "print(\"LIBRISPEECH_ROOT =\", LIBRISPEECH_ROOT)\n",
    "assert os.path.isdir(LIBRISPEECH_ROOT), \"dev-clean 경로를 확인하세요.\"\n",
    "\n",
    "# 오디오 파라미터\n",
    "SAMPLE_RATE = 16000  # LibriSpeech 기본 16kHz\n",
    "\n",
    "# DataLoader 파라미터\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0  # 윈도우면 0~2 정도로, 리눅스면 더 올려도 됨\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "\n",
    "# 특정 화자만 사용할 경우 지정 (None이면 전체 화자)\n",
    "# 예: speaker_list = [\"84\", \"174\"]\n",
    "speaker_list = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "class LibriSpeechSpeakerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    LibriSpeech dev-clean에서 화자 단위로 발화들을 로드한다.\n",
    "    각 item은 dict:\n",
    "        {\n",
    "          \"waveform\": Tensor shape (1, T) float32 [-1,1],\n",
    "          \"speaker_id\": str,\n",
    "          \"utt_path\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        target_sr: int = 16000,\n",
    "        speaker_list: Optional[List[str]] = None,\n",
    "        min_duration_sec: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        root_dir: dev-clean 경로\n",
    "        target_sr: 리샘플할 샘플레이트 (보통 16k)\n",
    "        speaker_list: 사용할 화자 ID 리스트. None이면 root_dir 내 모든 화자 ID 사용.\n",
    "        min_duration_sec: 너무 짧은 음성을 버리기 위한 최소 길이(초)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.target_sr = target_sr\n",
    "        self.min_duration_sec = min_duration_sec\n",
    "\n",
    "        # 1) 화자 폴더 수집 (폴더명이 전부 숫자인 것만 화자로 간주)\n",
    "        all_speakers = [\n",
    "            d for d in os.listdir(root_dir)\n",
    "            if os.path.isdir(os.path.join(root_dir, d)) and d.isdigit()\n",
    "        ]\n",
    "\n",
    "        if speaker_list is None:\n",
    "            self.speakers = sorted(all_speakers)\n",
    "        else:\n",
    "            # 교집합만 취함\n",
    "            self.speakers = sorted([s for s in all_speakers if s in speaker_list])\n",
    "\n",
    "        # 2) 각 화자에서 실제 오디오 파일(.flac/.wav) 경로 모으기\n",
    "        #    LibriSpeech는 일반적으로 spk_id/chapter_id/*.flac 형태\n",
    "        self.items = []\n",
    "        for spk in self.speakers:\n",
    "            spk_dir = os.path.join(root_dir, spk)\n",
    "            # 챕터 디렉토리들\n",
    "            for ch_name in os.listdir(spk_dir):\n",
    "                ch_dir = os.path.join(spk_dir, ch_name)\n",
    "                if not os.path.isdir(ch_dir):\n",
    "                    continue\n",
    "\n",
    "                # .wav / .flac 다 지원\n",
    "                wav_paths = glob.glob(os.path.join(ch_dir, \"*.wav\"))\n",
    "                flac_paths = glob.glob(os.path.join(ch_dir, \"*.flac\"))\n",
    "                audio_paths = sorted(wav_paths + flac_paths)\n",
    "\n",
    "                for ap in audio_paths:\n",
    "                    # 길이 필터링을 위해 일단 메타만 등록하고,\n",
    "                    # 실제 __getitem__에서 필요하면 필터하자.\n",
    "                    self.items.append({\n",
    "                        \"speaker_id\": spk,\n",
    "                        \"utt_path\": ap\n",
    "                    })\n",
    "\n",
    "        print(f\"[LibriSpeechSpeakerDataset] speakers: {len(self.speakers)}, utterances(raw): {len(self.items)}\")\n",
    "\n",
    "        # 사전 길이 필터를 적용해도 되지만, 여기서는 __getitem__에서 처리 실패 시 재시도 하기보단\n",
    "        # 그냥 __len__/__getitem__이 일관되게 동작하도록 그대로 둔다.\n",
    "        # (필요하면 나중에 pre-filter 로직 추가 가능)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def _load_audio(self, path: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        path에서 오디오 로딩하고 mono+resample까지 맞춰서 (1, T) float32 [-1,1] 반환\n",
    "        \"\"\"\n",
    "        wav, sr = torchaudio.load(path)  # wav: (C, T), float32 -1~1 범위일 가능성 높음\n",
    "        # 모노화\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "        # 리샘플\n",
    "        if sr != self.target_sr:\n",
    "            wav = torchaudio.functional.resample(wav, sr, self.target_sr)\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        meta = self.items[idx]\n",
    "        spk = meta[\"speaker_id\"]\n",
    "        path = meta[\"utt_path\"]\n",
    "\n",
    "        wav = self._load_audio(path)  # (1, T)\n",
    "\n",
    "        # 너무 짧으면(말 없는 부분 등) downstream에서 학습이 불안정할 수 있으므로 여기서 잘라낼 수 있다.\n",
    "        # 여기서는 min_duration_sec 이상만 보장하도록 잘라주는 정도만 (필요하다면)\n",
    "        min_len = int(self.min_duration_sec * self.target_sr)\n",
    "        if wav.shape[1] < min_len:\n",
    "            # 너무 짧다면 패딩 혹은 스킵 로직을 짤 수도 있다.\n",
    "            # 간단하게는 zero-pad\n",
    "            pad_len = min_len - wav.shape[1]\n",
    "            wav = torch.cat([wav, torch.zeros((1, pad_len), dtype=wav.dtype)], dim=1)\n",
    "\n",
    "        return {\n",
    "            \"waveform\": wav,       # (1, T_resampled)\n",
    "            \"speaker_id\": spk,     # string\n",
    "            \"utt_path\": path       # string\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibriSpeechSpeakerDataset] speakers: 40, utterances(raw): 2703\n"
     ]
    }
   ],
   "source": [
    "#데이터 체크\n",
    "# Dataset 생성\n",
    "dataset = LibriSpeechSpeakerDataset(\n",
    "    root_dir=LIBRISPEECH_ROOT,\n",
    "    target_sr=SAMPLE_RATE,\n",
    "    speaker_list=speaker_list,   # None이면 전체 speaker\n",
    "    min_duration_sec=0.5         # 너무 짧은 샘플은 최소 0.5초 길이로 패딩\n",
    ")\n",
    "\n",
    "# # DataLoader 생성\n",
    "# loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     num_workers=NUM_WORKERS,\n",
    "#     pin_memory=PIN_MEMORY,\n",
    "#     drop_last=False\n",
    "# )\n",
    "\n",
    "# # Sanity check: 한 배치만 뽑아서 정보 출력\n",
    "# batch = next(iter(loader))\n",
    "\n",
    "# print(\"=== Sanity Check ===\")\n",
    "# print(\"waveform.shape:\", batch[\"waveform\"].shape)  # (B, 1, T)\n",
    "# print(\"speaker_id:\", batch[\"speaker_id\"])         # list[str] 길이 B\n",
    "# print(\"utt_path[0]:\", batch[\"utt_path\"][0])\n",
    "\n",
    "# # wave 시각화 / 플레이 등은 나중에 노트북에서 직접 할 수 있음\n",
    "# # 예: torchaudio.display.waveplot 등\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sanity Check w/ collate_fn ===\n",
      "waveform.shape: torch.Size([4, 1, 514320])\n",
      "speaker_id example: ['2078', '2428', '5338', '84']\n",
      "utt_path[0]: c:\\project\\vcshield\\data\\train\\dev-clean\\2078\\142845\\2078-142845-0004.flac\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_with_padding(batch_list):\n",
    "    \"\"\"\n",
    "    batch_list는 __getitem__에서 나온 dict들의 리스트 (len = B)\n",
    "\n",
    "    목표:\n",
    "    - waveform들을 가장 긴 샘플 길이에 맞춰 zero-pad (right padding)\n",
    "    - speaker_id / utt_path는 리스트 그대로 유지\n",
    "    \"\"\"\n",
    "    wave_list = [item[\"waveform\"] for item in batch_list]  # [(1, T_i), ...]\n",
    "    spk_list = [item[\"speaker_id\"] for item in batch_list]\n",
    "    path_list = [item[\"utt_path\"] for item in batch_list]\n",
    "\n",
    "    # 가장 긴 길이 구하기\n",
    "    max_len = max(w.shape[1] for w in wave_list)\n",
    "\n",
    "    # pad 후 스택\n",
    "    padded = []\n",
    "    for w in wave_list:\n",
    "        if w.shape[1] < max_len:\n",
    "            pad_amount = max_len - w.shape[1]\n",
    "            w = F.pad(w, (0, pad_amount), mode=\"constant\", value=0.0)\n",
    "        padded.append(w)  # (1, max_len)\n",
    "    wave_tensor = torch.stack(padded, dim=0)  # (B, 1, max_len)\n",
    "\n",
    "    return {\n",
    "        \"waveform\": wave_tensor,\n",
    "        \"speaker_id\": spk_list,\n",
    "        \"utt_path\": path_list\n",
    "    }\n",
    "\n",
    "# collate_fn을 적용한 DataLoader\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_with_padding\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(\"=== Sanity Check w/ collate_fn ===\")\n",
    "print(\"waveform.shape:\", batch[\"waveform\"].shape)  # (B, 1, max_len_in_batch)\n",
    "print(\"speaker_id example:\", batch[\"speaker_id\"])\n",
    "print(\"utt_path[0]:\", batch[\"utt_path\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveform: torch.Size([4, 1, 193520])\n",
      "mel_db: torch.Size([4, 80, 756])\n",
      "speaker_id: ['174', '2428', '2086', '8842']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def build_mel_extractor(\n",
    "    sample_rate=16000,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_mels=80,\n",
    "    f_min=0.0,\n",
    "    f_max=8000.0,\n",
    "    power=1.0,\n",
    "    log_offset=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    반환 함수 wav_to_mel_db:\n",
    "      입력  : waveform (B,1,T) 또는 (1,T)\n",
    "      출력  : mel_batch (B, n_mels, time)\n",
    "    \"\"\"\n",
    "\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        n_mels=n_mels,\n",
    "        power=power,\n",
    "    )\n",
    "\n",
    "    def wav_to_mel_db(waveform: torch.Tensor):\n",
    "        \"\"\"\n",
    "        waveform: (B,1,T) or (1,T)\n",
    "        returns: (B, n_mels, time)\n",
    "        \"\"\"\n",
    "        single_input = False\n",
    "\n",
    "        # case: (1,T) -> (1,1,T)\n",
    "        if waveform.dim() == 2:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "            single_input = True\n",
    "\n",
    "        # sanity check\n",
    "        if waveform.dim() != 3 or waveform.shape[1] != 1:\n",
    "            raise ValueError(f\"Expected (B,1,T) or (1,T); got {tuple(waveform.shape)}\")\n",
    "\n",
    "        B = waveform.size(0)\n",
    "        mel_list = []\n",
    "\n",
    "        for b in range(B):\n",
    "            # waveform[b]: (1, T)\n",
    "            mel = mel_transform(waveform[b])  # (1, n_mels, time)\n",
    "            # squeeze channel dim -> (n_mels, time)\n",
    "            mel = mel.squeeze(0)\n",
    "\n",
    "            mel_db = torch.log(mel + log_offset)  # (n_mels, time)\n",
    "            mel_list.append(mel_db)\n",
    "\n",
    "        # stack -> (B, n_mels, time)\n",
    "        mel_batch = torch.stack(mel_list, dim=0)\n",
    "\n",
    "        if single_input:\n",
    "            mel_batch = mel_batch[0]  # (n_mels, time)\n",
    "\n",
    "        return mel_batch\n",
    "\n",
    "    return wav_to_mel_db\n",
    "\n",
    "\n",
    "# 빌드\n",
    "wav_to_mel_db = build_mel_extractor(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_mels=80,\n",
    "    f_min=0.0,\n",
    "    f_max=8000.0,\n",
    "    power=1.0,\n",
    "    log_offset=1e-6\n",
    ")\n",
    "\n",
    "# 테스트\n",
    "test_batch = next(iter(loader))\n",
    "test_wave = test_batch[\"waveform\"]      # (B,1,T)\n",
    "test_mel  = wav_to_mel_db(test_wave)    # 기대: (B,80,time)\n",
    "\n",
    "print(\"waveform:\", test_wave.shape)\n",
    "print(\"mel_db:\", test_mel.shape)\n",
    "print(\"speaker_id:\", test_batch[\"speaker_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G input shape: torch.Size([2, 80, 200])\n",
      "G output shape: torch.Size([2, 80, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, padding=padding)\n",
    "        self.norm = nn.BatchNorm2d(out_ch)\n",
    "        self.act  = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "def match_spatial(src, ref):\n",
    "    \"\"\"\n",
    "    src, ref: 4D 텐서 (B,C,H,W)\n",
    "    리턴: src와 ref 중 작은 쪽 크기로 둘을 맞춰서 (src_adj, ref_adj)를 돌려준다.\n",
    "    - 만약 H/W가 서로 다르면\n",
    "      중앙을 기준으로 잘라서 동일한 (H',W')로 맞춤\n",
    "    - src, ref 중 어느 쪽이 더 큰지/작은지는 각각 축마다 독립적으로 처리\n",
    "    \"\"\"\n",
    "    B1,C1,Hs,Ws = src.shape\n",
    "    B2,C2,Hr,Wr = ref.shape\n",
    "    assert B1 == B2, \"batch mismatch in match_spatial\"\n",
    "\n",
    "    # 최종 목표 크기\n",
    "    Ht = min(Hs, Hr)\n",
    "    Wt = min(Ws, Wr)\n",
    "\n",
    "    def center_crop(t, Ht, Wt):\n",
    "        _,_,H,W = t.shape\n",
    "        start_h = (H - Ht)//2\n",
    "        start_w = (W - Wt)//2\n",
    "        return t[:,:,start_h:start_h+Ht, start_w:start_w+Wt]\n",
    "\n",
    "    src_c = center_crop(src, Ht, Wt)\n",
    "    ref_c = center_crop(ref, Ht, Wt)\n",
    "    return src_c, ref_c\n",
    "\n",
    "class UNetLikeGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    입력:  mel_batch (B, n_mels, T)\n",
    "    출력:  adv_mel   (B, n_mels, T)\n",
    "    구조:  2D U-Net 스타일 (Freq x Time)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=64, n_mels=80):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "        # Encoder -----------------\n",
    "        self.enc1 = nn.Sequential(\n",
    "            ConvBlock(1, base_ch),\n",
    "            ConvBlock(base_ch, base_ch)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(base_ch, base_ch*2,\n",
    "                               kernel_size=4, stride=2, padding=1)  # /2\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            ConvBlock(base_ch*2, base_ch*2),\n",
    "            ConvBlock(base_ch*2, base_ch*2)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(base_ch*2, base_ch*4,\n",
    "                               kernel_size=4, stride=2, padding=1)  # /4\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            ConvBlock(base_ch*4, base_ch*4),\n",
    "            ConvBlock(base_ch*4, base_ch*4)\n",
    "        )\n",
    "\n",
    "        # Decoder -----------------\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*2,\n",
    "                                      kernel_size=4, stride=2, padding=1) # *2\n",
    "        self.dec2 = nn.Sequential(\n",
    "            ConvBlock(base_ch*4, base_ch*2),  # concat(e2)\n",
    "            ConvBlock(base_ch*2, base_ch*2)\n",
    "        )\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch*2, base_ch,\n",
    "                                      kernel_size=4, stride=2, padding=1) # *4\n",
    "        self.dec1 = nn.Sequential(\n",
    "            ConvBlock(base_ch*2, base_ch),    # concat(e1)\n",
    "            ConvBlock(base_ch, base_ch)\n",
    "        )\n",
    "\n",
    "        # Output ------------------\n",
    "        self.out_conv = nn.Conv2d(base_ch, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, mel_batch):\n",
    "        \"\"\"\n",
    "        mel_batch: (B, n_mels, T)\n",
    "        return:    adv_mel  : (B, n_mels, T)\n",
    "        \"\"\"\n",
    "        B, Freq, Time = mel_batch.shape\n",
    "\n",
    "        x = mel_batch.unsqueeze(1)  # (B,1,Freq,Time)\n",
    "\n",
    "        # ----- Encoder -----\n",
    "        e1 = self.enc1(x)           # (B,base_ch,Freq,Time)\n",
    "        d1 = self.down1(e1)         # (B,base_ch*2,~,~)\n",
    "\n",
    "        e2 = self.enc2(d1)          # (B,base_ch*2,~,~)\n",
    "        d2 = self.down2(e2)         # (B,base_ch*4,~,~)\n",
    "\n",
    "        bottleneck = self.enc3(d2)  # (B,base_ch*4,~,~)\n",
    "\n",
    "        # ----- Decoder -----\n",
    "        u2 = self.up2(bottleneck)   # (B,base_ch*2,~,~)\n",
    "        # match spatial size between u2 and e2\n",
    "        u2m, e2m = match_spatial(u2, e2)\n",
    "        cat2 = torch.cat([u2m, e2m], dim=1)  # (B,base_ch*4,~,~)\n",
    "        dec2 = self.dec2(cat2)               # (B,base_ch*2,~,~)\n",
    "\n",
    "        u1 = self.up1(dec2)                  # (B,base_ch,~,~)\n",
    "        # match spatial size between u1 and e1\n",
    "        u1m, e1m = match_spatial(u1, e1)\n",
    "        cat1 = torch.cat([u1m, e1m], dim=1)  # (B,base_ch*2,Freq,Time) ideally\n",
    "        dec1 = self.dec1(cat1)               # (B,base_ch,Freq,Time)\n",
    "\n",
    "        out_delta = self.out_conv(dec1)      # (B,1,Freq_dec,Time_dec)\n",
    "\n",
    "        # mel_batch.unsqueeze(1): (B,1,Freq_orig,Time_orig)\n",
    "        mel_as_img = mel_batch.unsqueeze(1)\n",
    "\n",
    "        # 최종 residual add 전에 공간 크기 맞추기\n",
    "        out_delta_adj, mel_as_img_adj = match_spatial(out_delta, mel_as_img)\n",
    "\n",
    "        adv_mel = mel_as_img_adj + out_delta_adj   # (B,1,Freq_final,Time_final)\n",
    "        adv_mel = adv_mel.squeeze(1)               # (B,Freq_final,Time_final)\n",
    "\n",
    "        import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, padding=padding)\n",
    "        self.norm = nn.BatchNorm2d(out_ch)\n",
    "        self.act  = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "def match_spatial(src, ref):\n",
    "    \"\"\"\n",
    "    src, ref: (B,C,H,W)\n",
    "    반환: (src_cropped, ref_cropped)\n",
    "    - 두 텐서를 동일한 (H',W')로 중앙 크롭해서 맞춘다.\n",
    "    - H',W'는 각 축별로 min(Hsrc,Href), min(Wsrc,Wref)\n",
    "    \"\"\"\n",
    "    B1,C1,Hs,Ws = src.shape\n",
    "    B2,C2,Hr,Wr = ref.shape\n",
    "    assert B1 == B2, \"batch mismatch in match_spatial\"\n",
    "\n",
    "    Ht = min(Hs, Hr)\n",
    "    Wt = min(Ws, Wr)\n",
    "\n",
    "    def center_crop(t, Ht, Wt):\n",
    "        _,_,H,W = t.shape\n",
    "        start_h = (H - Ht)//2\n",
    "        start_w = (W - Wt)//2\n",
    "        return t[:, :, start_h:start_h+Ht, start_w:start_w+Wt]\n",
    "\n",
    "    src_c = center_crop(src, Ht, Wt)\n",
    "    ref_c = center_crop(ref, Ht, Wt)\n",
    "    return src_c, ref_c\n",
    "\n",
    "\n",
    "class UNetLikeGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    입력:  mel_batch (B, n_mels, T)\n",
    "    출력:  adv_mel   (B, n_mels, T')  <- off-by-one 있을 수 있으므로 최종 크롭으로 원본과 최대한 맞춤\n",
    "    구조:  2D U-Net 스타일 (Freq x Time)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=64, n_mels=80):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "        # ----- Encoder -----\n",
    "        self.enc1 = nn.Sequential(\n",
    "            ConvBlock(1, base_ch),\n",
    "            ConvBlock(base_ch, base_ch)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(\n",
    "            base_ch, base_ch*2,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # /2\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            ConvBlock(base_ch*2, base_ch*2),\n",
    "            ConvBlock(base_ch*2, base_ch*2)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(\n",
    "            base_ch*2, base_ch*4,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # /4\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            ConvBlock(base_ch*4, base_ch*4),\n",
    "            ConvBlock(base_ch*4, base_ch*4)\n",
    "        )\n",
    "\n",
    "        # ----- Decoder -----\n",
    "        self.up2 = nn.ConvTranspose2d(\n",
    "            base_ch*4, base_ch*2,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # *2\n",
    "        self.dec2 = nn.Sequential(\n",
    "            ConvBlock(base_ch*4, base_ch*2),  # concat(e2)\n",
    "            ConvBlock(base_ch*2, base_ch*2)\n",
    "        )\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(\n",
    "            base_ch*2, base_ch,\n",
    "            kernel_size=4, stride=2, padding=1\n",
    "        )  # *4\n",
    "        self.dec1 = nn.Sequential(\n",
    "            ConvBlock(base_ch*2, base_ch),    # concat(e1)\n",
    "            ConvBlock(base_ch, base_ch)\n",
    "        )\n",
    "\n",
    "        # ----- Output -----\n",
    "        self.out_conv = nn.Conv2d(base_ch, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, mel_batch):\n",
    "        \"\"\"\n",
    "        mel_batch: (B, n_mels, T)\n",
    "        return:    adv_mel: (B, n_mels, T_adj)\n",
    "        \"\"\"\n",
    "        B, Freq, Time = mel_batch.shape  # Freq ~80, Time ~900~1000 가변\n",
    "\n",
    "        x = mel_batch.unsqueeze(1)  # (B,1,Freq,Time)\n",
    "\n",
    "        # ===== Encoder =====\n",
    "        e1 = self.enc1(x)           # (B,base_ch,Freq,Time)\n",
    "        d1 = self.down1(e1)         # (B,base_ch*2,~,~)\n",
    "\n",
    "        e2 = self.enc2(d1)          # (B,base_ch*2,~,~)\n",
    "        d2 = self.down2(e2)         # (B,base_ch*4,~,~)\n",
    "\n",
    "        bottleneck = self.enc3(d2)  # (B,base_ch*4,~,~)\n",
    "\n",
    "        # ===== Decoder =====\n",
    "        u2 = self.up2(bottleneck)   # (B,base_ch*2,~,~)\n",
    "        u2m, e2m = match_spatial(u2, e2)\n",
    "        cat2 = torch.cat([u2m, e2m], dim=1)   # (B,base_ch*4,~,~)\n",
    "        dec2 = self.dec2(cat2)                # (B,base_ch*2,~,~)\n",
    "\n",
    "        u1 = self.up1(dec2)                   # (B,base_ch,~,~)\n",
    "        u1m, e1m = match_spatial(u1, e1)\n",
    "        cat1 = torch.cat([u1m, e1m], dim=1)   # (B,base_ch*2,~,~)\n",
    "        dec1 = self.dec1(cat1)                # (B,base_ch,~,~)\n",
    "\n",
    "        out_delta = self.out_conv(dec1)       # (B,1,H_out,W_out)\n",
    "\n",
    "        # ===== Residual add with original mel =====\n",
    "        mel_as_img = mel_batch.unsqueeze(1)   # (B,1,Freq,Time)\n",
    "\n",
    "        out_delta_adj, mel_as_img_adj = match_spatial(out_delta, mel_as_img)\n",
    "        # 이제 두 텐서의 (H,W)이 동일해졌음\n",
    "\n",
    "        adv_mel = mel_as_img_adj + out_delta_adj  # (B,1,Ht,Wt)\n",
    "        adv_mel = adv_mel.squeeze(1)              # (B,Ht,Wt)\n",
    "\n",
    "        return adv_mel\n",
    "\n",
    "\n",
    "\n",
    "# 간단하게 모델 만들어보고 shape 통과 확인\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "G = UNetLikeGenerator(base_ch=64, n_mels=80).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    dummy_in = torch.randn(2, 80, 200).to(device)  # (B, n_mels, T_mel)\n",
    "    dummy_out = G(dummy_in)\n",
    "print(\"G input shape:\", dummy_in.shape)\n",
    "print(\"G output shape:\", dummy_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel_clean: torch.Size([4, 80, 457])\n",
      "mel_adv: torch.Size([4, 80, 456])\n"
     ]
    }
   ],
   "source": [
    "# 한 스텝 예시 (forward 경로 점검)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "\n",
    "wave = batch[\"waveform\"].to(device)  # (B,1,T)\n",
    "spk_ids = batch[\"speaker_id\"]\n",
    "paths = batch[\"utt_path\"]\n",
    "\n",
    "# 1. waveform -> mel (CPU에서 mel 추출한 뒤 device로 올리도록 했었는데\n",
    "#    torchaudio는 GPU안써도 괜찮으니 그냥 CPU에서 돌리고 나중에 to(device))\n",
    "mel_clean_cpu = wav_to_mel_db(wave.cpu())   # (B,80,Tmel) on CPU\n",
    "mel_clean = mel_clean_cpu.to(device)\n",
    "\n",
    "print(\"mel_clean:\", mel_clean.shape)  # 기대: (B,80,Tmel)\n",
    "\n",
    "# 2. pass through G\n",
    "mel_adv = G(mel_clean)  # (B,80,Tmel)\n",
    "\n",
    "print(\"mel_adv:\", mel_adv.shape)\n",
    "\n",
    "# 3. (미래 작업 TODO)\n",
    "#   - mel_adv를 디노이저 D에 통과 -> mel_denoised\n",
    "#   - vocoder V(mel_denoised) -> wav_adv\n",
    "#   - speaker encoder E(wav_adv)와 E(wave_clean) 비교해서 loss\n",
    "#   - L_content, L_resist 계산\n",
    "#   - backward/optimizer.step()\n",
    "\n",
    "# 지금 단계의 목표:\n",
    "#   데이터로더 -> mel 변환 -> G forward 가 오류 없이 shape 일관성 있게 흐르는지 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D input: torch.Size([4, 80, 992]) D output: torch.Size([4, 80, 992])\n"
     ]
    }
   ],
   "source": [
    "#디노이저\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleDenoiser(nn.Module):\n",
    "    \"\"\"\n",
    "    멜 스펙 (B, n_mels, T) -> (B, n_mels, T)\n",
    "    시간 방향으로만 살짝 smoothing을 넣어서\n",
    "    '노이즈 정화 시도'를 근사한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=5):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        # depthwise conv1d 흉내: 각 mel 채널별로 동일 커널 적용\n",
    "        # weight shape: (C_out, C_in/groups, K)\n",
    "        # groups = n_mels -> mel 채널별 독립 필터\n",
    "        # 우리는 runtime에 weight를 생성해서 F.conv1d로 처리할 예정 (학습 X, 고정)\n",
    "        # 이유: 간단히 deterministic blur만 하고 싶기 때문\n",
    "        # 즉, forward 안에서 커널을 만들어 쓴다.\n",
    "\n",
    "    def forward(self, mel):\n",
    "        \"\"\"\n",
    "        mel: (B, n_mels, T)\n",
    "        return: (B, n_mels, T)\n",
    "        \"\"\"\n",
    "        B, C, T = mel.shape\n",
    "        k = self.kernel_size\n",
    "\n",
    "        # 평균커널: (C,1,k)\n",
    "        kernel = torch.ones(C, 1, k, device=mel.device, dtype=mel.dtype) / k\n",
    "\n",
    "        # padding 'reflect' or 'replicate' 등으로 시간 길이 유지\n",
    "        pad = (k // 2, k // 2)\n",
    "        mel_padded = F.pad(mel, pad=pad, mode='reflect')  # (B,C,T+padL+padR)\n",
    "\n",
    "        # depthwise conv1d\n",
    "        mel_blur = F.conv1d(\n",
    "            mel_padded,          # (B,C,T+pad)\n",
    "            kernel,              # (C,1,k)\n",
    "            bias=None,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=C\n",
    "        )  # (B,C,T)\n",
    "\n",
    "        return mel_blur\n",
    "\n",
    "D = SimpleDenoiser(kernel_size=5).to(device)\n",
    "\n",
    "# quick shape test\n",
    "dummy = torch.randn(4, 80, 992).to(device)\n",
    "out_d = D(dummy)\n",
    "print(\"D input:\", dummy.shape, \"D output:\", out_d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E input: torch.Size([4, 80, 992]) E output(emb): torch.Size([4, 192])\n"
     ]
    }
   ],
   "source": [
    "#ECAPA(임베딩을 뽑는 네트워크인데 임시로 흉내냄)\n",
    "class DummySpeakerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    mel (B, 80, T) -> speaker embedding (B, emb_dim)\n",
    "    아주 단순한 conv + global pooling + linear.\n",
    "    실제로는 ECAPA 등으로 대체할 예정.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=80, emb_dim=192):\n",
    "        super().__init__()\n",
    "        # conv over time\n",
    "        self.conv1 = nn.Conv1d(in_ch, 128, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(128, 128, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n",
    "        self.norm  = nn.BatchNorm1d(256)\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "        self.proj  = nn.Linear(256, emb_dim)\n",
    "\n",
    "    def forward(self, mel):\n",
    "        \"\"\"\n",
    "        mel: (B, 80, T)\n",
    "        return: (B, emb_dim)\n",
    "        \"\"\"\n",
    "        x = self.conv1(mel)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)          # (B,256,T)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        # global average pool over time\n",
    "        x = torch.mean(x, dim=2)   # (B,256)\n",
    "\n",
    "        x = self.proj(x)           # (B,emb_dim)\n",
    "        # normalize for cosine stability\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "E = DummySpeakerEncoder(in_ch=80, emb_dim=192).to(device)\n",
    "\n",
    "# quick shape test\n",
    "with torch.no_grad():\n",
    "    emb_test = E(dummy)  # dummy: (4,80,992)\n",
    "print(\"E input:\", dummy.shape, \"E output(emb):\", emb_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_content: 0.3105723261833191\n",
      "L_resist : -0.9993799328804016\n",
      "L_total  : -0.6888076066970825\n",
      "✅ backward/step까지 완료\n"
     ]
    }
   ],
   "source": [
    "#손실 계산\n",
    "\n",
    "def match_mel_for_loss(mel_a, mel_b):\n",
    "    \"\"\"\n",
    "    mel_a: (B, F, T)\n",
    "    mel_b: (B, F, T)\n",
    "    return: (a_crop, b_crop) with identical (F', T')\n",
    "    둘 중 더 작은 쪽에 중앙 크롭 맞춘다.\n",
    "    \"\"\"\n",
    "    A4 = mel_a.unsqueeze(1)  # (B,1,F,T)\n",
    "    B4 = mel_b.unsqueeze(1)  # (B,1,F,T)\n",
    "\n",
    "    A4c, B4c = match_spatial(A4, B4)  # (B,1,F',T'), (B,1,F',T')\n",
    "\n",
    "    A3c = A4c.squeeze(1)  # (B,F',T')\n",
    "    B3c = B4c.squeeze(1)  # (B,F',T')\n",
    "\n",
    "    return A3c, B3c\n",
    "\n",
    "\n",
    "# 하이퍼파라미터\n",
    "lambda_c = 1.0   # content 보존 가중치\n",
    "lambda_r = 1.0   # resist(식별 방해) 가중치\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "\n",
    "# === 1 step example ===\n",
    "batch = next(iter(loader))\n",
    "wave = batch[\"waveform\"].to(device)        # (B,1,T)\n",
    "\n",
    "# 1. waveform -> mel_clean (CPU에서 mel 추출 후 device로 옮김)\n",
    "mel_clean_cpu = wav_to_mel_db(wave.cpu())  # (B,80,Tmel) on CPU\n",
    "mel_clean = mel_clean_cpu.to(device)       # (B,80,Tmel)\n",
    "\n",
    "# 2. 생성기 통과\n",
    "mel_adv = G(mel_clean)                     # (B,80,Tmel)\n",
    "\n",
    "# 3. 디노이저(=공격자 정화 시뮬레이션)\n",
    "mel_denoised = D(mel_adv)                  # (B,80,Tmel)\n",
    "\n",
    "# 4. 화자 임베딩 (placeholder)\n",
    "emb_orig = E(mel_clean)                    # (B,emb_dim)\n",
    "emb_denoised = E(mel_denoised)             # (B,emb_dim)\n",
    "\n",
    "# 5. 손실 계산\n",
    "# (a) content loss: adv mel이 clean mel과 너무 달라지지 않도록\n",
    "mel_adv_crop, mel_clean_crop = match_mel_for_loss(mel_adv, mel_clean)\n",
    "L_content = F.l1_loss(mel_adv_crop, mel_clean_crop)\n",
    "\n",
    "# (b) resist loss: 디노이즈 후 임베딩이 원본 임베딩과 닮지 않도록\n",
    "# cosine similarity를 계산하고, 그걸 낮추고 싶다 → 음수로 곱해주면 된다.\n",
    "cos_sim = torch.sum(emb_orig * emb_denoised, dim=1)  # (B,)\n",
    "L_resist = - torch.mean(cos_sim)  # 유사도가 낮아질수록(멀어질수록) 낮은 loss가 되게 부호 설정\n",
    "\n",
    "L_total = lambda_c * L_content + lambda_r * L_resist\n",
    "\n",
    "print(\"L_content:\", float(L_content.item()))\n",
    "print(\"L_resist :\", float(L_resist.item()))\n",
    "print(\"L_total  :\", float(L_total.item()))\n",
    "\n",
    "# 6. backward & optimizer step\n",
    "optimizer.zero_grad()\n",
    "L_total.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"✅ backward/step까지 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel_clean.shape      : torch.Size([4, 80, 761])\n",
      "mel_adv.shape        : torch.Size([4, 80, 760])\n",
      "mel_adv_crop.shape   : torch.Size([4, 80, 760])\n",
      "mel_clean_crop.shape : torch.Size([4, 80, 760])\n",
      "emb_orig.shape       : torch.Size([4, 192])\n",
      "emb_denoised.shape   : torch.Size([4, 192])\n",
      "L_content: 0.4890805780887604\n",
      "L_resist : -0.9991296529769897\n",
      "L_total  : -0.5100491046905518\n",
      "✅ backward/step까지 완료\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1) mel 크롭 유틸\n",
    "def match_mel_for_loss(mel_a, mel_b):\n",
    "    \"\"\"\n",
    "    mel_a: (B,F,T)\n",
    "    mel_b: (B,F,T)\n",
    "    return: (a_crop, b_crop) with same (F',T')\n",
    "    \"\"\"\n",
    "    A4 = mel_a.unsqueeze(1)  # (B,1,F,T)\n",
    "    B4 = mel_b.unsqueeze(1)  # (B,1,F,T)\n",
    "    A4c, B4c = match_spatial(A4, B4)\n",
    "    A3c = A4c.squeeze(1)     # (B,F',T')\n",
    "    B3c = B4c.squeeze(1)     # (B,F',T')\n",
    "    return A3c, B3c\n",
    "\n",
    "# ---------------- Hyperparams / Optimizer ----------------\n",
    "lambda_c = 1.0   # content weight\n",
    "lambda_r = 1.0   # resist weight\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "\n",
    "# ---------------- One training step prototype ----------------\n",
    "batch = next(iter(loader))\n",
    "wave = batch[\"waveform\"].to(device)            # (B,1,T)\n",
    "\n",
    "# waveform -> mel_clean (still on CPU for torchaudio, then to device)\n",
    "mel_clean_cpu = wav_to_mel_db(wave.cpu())      # (B,80,Tmel_clean)\n",
    "mel_clean = mel_clean_cpu.to(device)           # (B,80,Tmel_clean)\n",
    "\n",
    "# G forward\n",
    "mel_adv = G(mel_clean)                         # (B,80,Tmel_adv)\n",
    "\n",
    "# D (denoiser / purification simulation)\n",
    "mel_denoised = D(mel_adv)                      # (B,80,Tmel_adv)  (same T as mel_adv by design)\n",
    "\n",
    "# Speaker embeddings (placeholder E on mel)\n",
    "emb_orig      = E(mel_clean)                   # (B,emb_dim)\n",
    "emb_denoised  = E(mel_denoised)                # (B,emb_dim)\n",
    "\n",
    "# Content loss: crop mel_adv and mel_clean to same spatial size\n",
    "mel_adv_crop, mel_clean_crop = match_mel_for_loss(mel_adv, mel_clean)\n",
    "L_content = F.l1_loss(mel_adv_crop, mel_clean_crop)\n",
    "\n",
    "# Resist loss: we want cosine similarity to go DOWN\n",
    "cos_sim = torch.sum(emb_orig * emb_denoised, dim=1)  # (B,)\n",
    "L_resist = - torch.mean(cos_sim)\n",
    "\n",
    "L_total = lambda_c * L_content + lambda_r * L_resist\n",
    "\n",
    "print(\"mel_clean.shape      :\", mel_clean.shape)\n",
    "print(\"mel_adv.shape        :\", mel_adv.shape)\n",
    "print(\"mel_adv_crop.shape   :\", mel_adv_crop.shape)\n",
    "print(\"mel_clean_crop.shape :\", mel_clean_crop.shape)\n",
    "print(\"emb_orig.shape       :\", emb_orig.shape)\n",
    "print(\"emb_denoised.shape   :\", emb_denoised.shape)\n",
    "print(\"L_content:\", float(L_content.item()))\n",
    "print(\"L_resist :\", float(L_resist.item()))\n",
    "print(\"L_total  :\", float(L_total.item()))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "L_total.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"✅ backward/step까지 완료\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
